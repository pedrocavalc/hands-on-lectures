{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Suponha que você tenha vários classificadores treinados exatamente nos mesmos dados com 95% de acurácia. É possível combinar os modelos para obter uim melhor? Se sim, como?\n",
    "\n",
    "R: Sim é possível, no entanto, somente é possível se os classificadores forem diferentes entre si por exemplo: Uma random forest e uma MLP e SVM. É possível atingir isso utilizando técnicas de votação, como o hard vote ou o soft vote. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Qual a diferença entre os classificadores de hard vote e soft voting?\n",
    "\n",
    "R: Os classificares hard voting utilizam abordagem de utilizar a classe mais predita entre os classificadores como a classe final. Exemplo: Temos 5 classificadores, 4 classificaram uma amostra como positiva e 1 como negativa, logo a classe final será positiva. Já os classificadores soft voting utilizan a abordagem de utilizar a classe que teve a maior probabilidade média entre os classificadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) É possível paralelizar o treinamento de um método bagging? E quanto aos outros?\n",
    "\n",
    "R: O método bagging consiste em utilizar pequenos subconjuntos aleatórios do dataset (com substituição, sem substituição se caracteriza o pasting), para treinar vários preditores do mesmo tipo. Sendo assim, cada preditor tem seu treinamento independente um do outro. O pasting da mesma forma pode ser paralelizado. Já o boosting e o stacking não podem ser paralizados devido a sua característica de usar preditores em cascata, em cima das formas de erros do preditor passado. O floresta randômica já é um paralelismo das árvores de decisão em que ele separa o conjunto de treinamento em subconjuntos então é sim possível."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Qual é a vantagem da Avaliação out of bag?\n",
    "\n",
    "R: A avaliação out-of-bag utiliza as instâncias que ficaram de fora do treinamento de cada um dos preditores do método bagging(ou pasting) para fazer a validação de cada um dos preditores. Sendo assim, podemos utiliza-lo para avaliar o modelo sem que seja necessário construir um conjunto de validação."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Como as ávores extras conseguem ser ainda mais aleatórias que as florestas randômicas? Elas são mais rápidas ou mais lentas de treinar?\n",
    "\n",
    "R: As árvores extras consistem em utilizar limiares aletórios para a separação das características. Ao invés de procurar a melhor divisão entre as amostras e as características elas utilizam limiares aleatórios. Sendo assim, o treinamento é mais veloz, devido ao não precisar procurar pela melhor separação das amostras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Se o Adaboost se subajustar aos dados, que parâmetros podem ser mudados?\n",
    "\n",
    "R: Pode-se aumentar o numero de estimadores assim como retirar algumas regularizações do modelo base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Caso o agrupamento do gradientboost se sobreajustar aos dados, deve-se aumentar ou reduzir a learning rate?\n",
    "\n",
    "R: Diminuir a learning rate para que mais preditores sejam treinados e assim melhorar a generalização dos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Treine diversos classificadores no conjunto MNIST e após isso faça um agrupamento que supere os classificadores individuais.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "carregando o dataset e split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    mnist.data, mnist.target, test_size=10000, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=10000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  11., 213., 193.,  30.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  92.,\n",
       "       252., 233.,  30.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  31.,  31.,   0.,   0., 173., 253., 102.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0., 123., 233., 151.,   0.,   0.,\n",
       "       253., 252., 102.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  21.,\n",
       "       173., 253., 244., 122.,   0., 102., 254., 253., 102.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,  41., 223., 253., 252., 162.,   0.,   0.,\n",
       "       183., 253., 252.,  20.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,  82., 214.,\n",
       "       253., 254., 131.,   0.,   0.,  51., 253., 254., 192.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,  82., 243., 253., 252., 131.,  10.,   0.,   0.,\n",
       "       132., 252., 253.,  70.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 123., 254., 253.,\n",
       "       244.,  81.,   0.,   0.,   0.,  21., 214., 253., 254.,  71.,  31.,\n",
       "        51.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0., 163., 243., 253., 252.,  81.,   0.,   0.,   0.,  62.,\n",
       "       162., 253., 252., 253., 232., 233., 252., 123.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,  31., 193., 254., 253., 254.,\n",
       "       253., 254., 253., 254., 253., 254., 253., 254., 253., 254., 253.,\n",
       "       255., 253., 183.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,  51., 252., 253., 252., 253., 252., 253., 252., 253., 252.,\n",
       "       253., 252., 253., 252., 233., 151., 151., 111.,  20.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 123., 203., 203.,\n",
       "       203., 162., 123., 162., 102., 102., 193., 253., 254., 131.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0., 233., 252., 253.,  50.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 254., 253., 203.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0., 253., 252., 203.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0., 123., 255., 253., 142.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0., 203., 253., 252., 102.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,  31., 233., 255., 253.,\n",
       "        41.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,  10., 212., 253., 171.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "         0.,   0.,   0.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "treinando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_clf = MLPClassifier(random_state=42)\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=10)\n",
    "svc_clf =  LinearSVC(max_iter=100, tol=20,random_state=42)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100,random_state=42)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=100,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [mlp_clf, knn_clf, svc_clf, rf_clf, et_clf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O classificador MLPClassifier(random_state=42) esta treinando\n",
      "0.9602\n",
      "O classificador KNeighborsClassifier(n_neighbors=10) esta treinando\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda3/envs/tf2/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:230: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9663\n",
      "O classificador LinearSVC(max_iter=100, random_state=42, tol=20) esta treinando\n",
      "0.859\n",
      "O classificador RandomForestClassifier(random_state=42) esta treinando\n",
      "0.9692\n",
      "O classificador ExtraTreesClassifier(random_state=42) esta treinando\n",
      "0.9715\n"
     ]
    }
   ],
   "source": [
    "for clf in classifiers:\n",
    "    print(f'O classificador {clf} esta treinando')\n",
    "    clf.fit(X_train,y_train)\n",
    "    print(clf.score(X_val, y_val))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda3/envs/tf2/lib/python3.8/site-packages/sklearn/neighbors/_classification.py:230: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9728\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vot_clf = VotingClassifier(estimators=[('mlp',mlp_clf),('knn',knn_clf), ('SVC',svc_clf), ('rf',rf_clf), ('et', et_clf)],voting='hard')\n",
    "vot_clf.fit(X_train,y_train)\n",
    "print(vot_clf.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vot_clf = vot_clf.params(svc='drop')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b463cf52ca8196c9c2a7ed6997d0666bf4884dc439b687c01213b9631ac3d5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
