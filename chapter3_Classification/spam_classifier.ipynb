{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DOWNLOAD_ROOT = \"https://spamassassin.apache.org/old/publiccorpus/\"\n",
    "HAM_URL = DOWNLOAD_ROOT + '20030228_easy_ham.tar.bz2'\n",
    "SPAM_PATH = os.path.join(\"datasets\",\"spam\")\n",
    "SPAM_URL = DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tarfile\n",
    "def fetch_housing_data(spam_url = SPAM_URL,spam_path = SPAM_PATH, ham_url = HAM_URL):\n",
    "    os.makedirs(spam_path, exist_ok=True)\n",
    "    for file_name, url in (('ham.tar.bz2',ham_url),(\"spam.tar.bz2\",spam_url)):\n",
    "        tgz_path = os.path.join(spam_path, file_name)\n",
    "        urllib.request.urlretrieve(url,tgz_path)\n",
    "        spamtgz = tarfile.open(tgz_path)\n",
    "        spamtgz.extractall(path=spam_path)\n",
    "        spamtgz.close()\n",
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting from MIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2501 501\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "ham_files = glob.glob('datasets/spam/easy_ham/*')\n",
    "spam_files = glob.glob('datasets/spam/spam/*')\n",
    "print(len(ham_files),len(spam_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'datasets/spam'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SPAM_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def load_emails(is_spam,file_name, spam_path = SPAM_PATH):\n",
    "    directory = \"spam\" if is_spam else \"easy_ham\"\n",
    "    with open(os.path.join(spam_path, directory ,file_name),'rb') as f:\n",
    "        return email.parser.BytesParser(policy=email.policy.default).parse(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hams_emails = [load_emails(is_spam=False,file_name= name.split('/')[-1]) for name in ham_files]\n",
    "spams_emails = [load_emails(is_spam=True,file_name= name.split('/')[-1]) for name in spam_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://news.bbc.co.uk/1/hi/entertainment/showbiz/2308581.stm\n",
      "\n",
      "Tuesday, 8 October, 2002, 07:55 GMT 08:55 UK\n",
      "Lennon killer seeks parole again\n",
      "\n",
      "The man who shot dead former Beatle John Lennon is making another bid for\n",
      "early release from prison - the day before what would have been Lennon's\n",
      "62nd birthday.\n",
      "Mark David Chapman, 47, was jailed for life after he admitted killing the\n",
      "superstar outside his New York apartment building in 1980.\n",
      "It is the second time in two years that Chapman has sought parole from\n",
      "Attica state prison.\n",
      "At a 2000 hearing, he argued that he was no longer a danger to society and\n",
      "had overcome the psychological problems which led him to shoot the\n",
      "ex-Beatle.\n",
      "Chapman had said that a voice in his head told him to shoot the star.\n",
      "Shot dead\n",
      "Lennon was shot four times as he emerged from a limousine outside his New\n",
      "York City apartment on 8 December 1980.\n",
      "He and his wife Yoko Ono were returning from a late-night recording session\n",
      "during which time they had been working on Walking on Thin Ice.\n",
      "Only hours before the shooting, Chapman - who had come to New York from\n",
      "Hawaii - was photographed with the singer outside the same building as\n",
      "Lennon signed a copy of his album Double Fantasy for him.\n",
      "The killer said Lennon had been just \"a picture on an album cover\" to him\n",
      "before the shooting.\n",
      "'Deserved death'\n",
      "Chapman has said that he should have received the death penalty for his\n",
      "crime.\n",
      "Lennon's widow told the 2000 parole hearing that she would not feel safe if\n",
      "Chapman were released.\n",
      "Lennon's songwriting partnership with Paul McCartney propelled the\n",
      "Liverpool-based pop group to international stardom and unparalleled\n",
      "commercial success.\n",
      "The Beatles front man, peace campaigner, and all-round iconoclast, would\n",
      "have been 62 on Wednesday.\n",
      "\n",
      "------------------------ Yahoo! Groups Sponsor ---------------------~-->\n",
      "Plan to Sell a Home?\n",
      "http://us.click.yahoo.com/J2SnNA/y.lEAA/MVfIAA/7gSolB/TM\n",
      "---------------------------------------------------------------------~->\n",
      "\n",
      "To unsubscribe from this group, send an email to:\n",
      "forteana-unsubscribe@egroups.com\n",
      "\n",
      " \n",
      "\n",
      "Your use of Yahoo! Groups is subject to http://docs.yahoo.com/info/terms/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(hams_emails[1].get_content().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "\n",
    "    payload  = email.get_payload()\n",
    "    if isinstance(payload,list):\n",
    "        return \"multipart{}\".format(\", \".join([get_email_structure(sub_email) for sub_email in payload]))\n",
    "    else:\n",
    "        return email.get_content_type()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    \n",
    "    return structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 2409),\n",
       " ('multiparttext/plain, application/pgp-signature', 66),\n",
       " ('multiparttext/plain, text/html', 8),\n",
       " ('multiparttext/plain, text/plain', 4),\n",
       " ('multiparttext/plain', 3),\n",
       " ('multiparttext/plain, application/octet-stream', 2),\n",
       " ('multiparttext/plain, application/x-pkcs7-signature', 1),\n",
       " ('multiparttext/plain, video/mng', 1),\n",
       " ('multiparttext/plain, multiparttext/plain, text/plain, multipartmultiparttext/plain, application/x-pkcs7-signature',\n",
       "  1),\n",
       " ('multiparttext/plain, application/x-java-applet', 1),\n",
       " ('multiparttext/plain, multiparttext/plain, text/plain, text/rfc822-headers',\n",
       "  1),\n",
       " ('multiparttext/plain, application/ms-tnef, text/plain', 1),\n",
       " ('multipartmultiparttext/plain, text/plain, text/plain, application/pgp-signature',\n",
       "  1),\n",
       " ('multiparttext/plain, text/enriched', 1),\n",
       " ('multiparttext/plain, multiparttext/plain', 1)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(hams_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text/plain', 219),\n",
       " ('text/html', 183),\n",
       " ('multiparttext/plain, text/html', 45),\n",
       " ('multiparttext/html', 20),\n",
       " ('multiparttext/plain', 19),\n",
       " ('multipartmultiparttext/html', 5),\n",
       " ('multiparttext/plain, image/jpeg', 3),\n",
       " ('multiparttext/html, application/octet-stream', 2),\n",
       " ('multipart/alternative', 1),\n",
       " ('multipartmultiparttext/plain, text/html, image/gif', 1),\n",
       " ('multiparttext/html, text/plain', 1),\n",
       " ('multiparttext/plain, application/octet-stream', 1),\n",
       " ('multipartmultiparttext/html, application/octet-stream, image/jpeg', 1)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spams_emails).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return-Path : <antheaygd@chinchilla.freeserve.co.uk>\n",
      "Delivered-To : zzzz@localhost.spamassassin.taint.org\n",
      "Received : from localhost (jalapeno [127.0.0.1])\tby zzzzason.org (Postfix) with ESMTP id B141E16F03\tfor <zzzz@localhost>; Wed, 25 Sep 2002 00:17:07 +0100 (IST)\n",
      "Received : from jalapeno [127.0.0.1]\tby localhost with IMAP (fetchmail-5.9.0)\tfor zzzz@localhost (single-drop); Wed, 25 Sep 2002 00:17:07 +0100 (IST)\n",
      "Received : from webnote.net (mail.webnote.net [193.120.211.219]) by    dogma.slashnull.org (8.11.6/8.11.6) with ESMTP id g8ONBYC27124 for    <zzzz@jmason.org>; Wed, 25 Sep 2002 00:11:34 +0100\n",
      "Received : from mail1.codetel.net.do (m30exfe1.codetel.net.do    [196.3.81.56]) by webnote.net (8.9.3/8.9.3) with ESMTP id AAA21947;    Wed, 25 Sep 2002 00:12:07 +0100\n",
      "Received : from mail-in.pol.net.uk ([64.32.101.241]) by    mail1.codetel.net.do with Microsoft SMTPSVC(5.0.2195.5329); Tue,    24 Sep 2002 19:11:08 -0400\n",
      "From : Ingrid Marksberry <antheaygd@chinchilla.freeserve.co.uk>\n",
      "To : MARKET INTERESTS <7869@xo.com>\n",
      "Subject : FW: FOCUS ON VALUE\n",
      "MIME-Version : 1.0\n",
      "Message-Id : <M30EXFE1YIcVqeavxhg0005bdac@mail1.codetel.net.do>\n",
      "X-Originalarrivaltime : 24 Sep 2002 23:11:10.0850 (UTC) FILETIME=[ABC6EE20:01C2641F]\n",
      "Date : Tue, 24 Sep 2002 19:11:10 -0400\n",
      "Content-Type : text/html; charset=\"iso-8859-1\"\n",
      "Content-Transfer-Encoding : quoted-printable\n"
     ]
    }
   ],
   "source": [
    "for header, value in spams_emails[0].items():\n",
    "    print(header,\":\",value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Free Shipping on all orders at Blair.com'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spams_emails[2][\"Subject\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = np.array(hams_emails + spams_emails, dtype= object)\n",
    "y = np.array([0] * len(hams_emails) + [1] * len(spams_emails))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, y,test_size= 0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_plain_text(html):\n",
    "    text = re.sub('.*?', '', html, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return unescape(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<meta http-equiv=\"content-type\" content=\"text/html; charset=euc-kr\">\n",
      "<title>요즘 뜨는 직종 Best 5 : 금융 / IT / 방송 / 뷰티 / 인테리어</title>\n",
      "<meta name=\"generator\" content=\"Namo WebEditor v5.0\">\n",
      "</head>\n",
      "<BODY text=black vLink=purple aLink=red link=blue bgColor=white>\n",
      "<TABLE cellSpacing=0 cellPadding=0 width=672 border=0>\n",
      "<TBODY>\n",
      "<TR>\n",
      "<TD width=7 bgColor=#ffffff><IMG height=1\n",
      "src=\"http://image.hanmail.net/hanmail/general/trans.gif\" width=7><BR></TD>\n",
      "<TD vAlign=top width=\"99%\" bgColor=#ffffff><IMG height=105\n",
      "src=\"http://image.hanmail.net/hanmail/s_img/recruit/sp_top01.gif\"\n",
      "width=221><IMG height=105\n",
      "src=\"http://image.hanmail.net/hanmail/s_img/recruit/sp_top02.gif\"\n",
      "width=442>\n",
      "<TABLE cellSpacing=0 cellPadding=0 width=\"100%\" border=0>\n",
      "<TBODY>\n",
      "<TR>\n",
      "<TD width=5><IMG height=8\n",
      "src=\"http://image.hanmail.net/hanmail/s_img/recruit/g_raound01.gif\"\n",
      "width=5><BR></TD>\n",
      "<TD width=\"97%\" bgColor=#cbd940></TD>\n",
      "<TD width=13><IMG height=8\n",
      "src=\"http://image.hanmail.net/hanmail/s_img/recruit/g_raound02.gif\"\n",
      "wid ...\n"
     ]
    }
   ],
   "source": [
    "html_spam_emails = [email for email in X_train[y_train==1]\n",
    "                    if get_email_structure(email) == \"text/html\"]\n",
    "sample_html_spam = html_spam_emails[7]\n",
    "print(sample_html_spam.get_content().strip()[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HYPERLINK  ...\n"
     ]
    }
   ],
   "source": [
    "print(html_to_plain_text(sample_html_spam.get_content())[:1000], \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_text(email):\n",
    "    html = None\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if not ctype in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except: # in case of encoding issues\n",
    "            content = str(part.get_payload())\n",
    "        if ctype == \"text/plain\":\n",
    "            return content\n",
    "        else:\n",
    "            html = content\n",
    "    if html:\n",
    "        return html_to_plain_text(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HYPERLINK  ...\n"
     ]
    }
   ],
   "source": [
    "print(email_to_text(sample_html_spam)[:100], \"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computations => comput\n",
      "Computation => comput\n",
      "Computing => comput\n",
      "Computed => comput\n",
      "Compute => comput\n",
      "Compulsive => compuls\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import nltk\n",
    "\n",
    "    stemmer = nltk.PorterStemmer()\n",
    "    for word in (\"Computations\", \"Computation\", \"Computing\", \"Computed\", \"Compute\", \"Compulsive\"):\n",
    "        print(word, \"=>\", stemmer.stem(word))\n",
    "except ImportError:\n",
    "    print(\"Error: stemming requires the NLTK module.\")\n",
    "    stemmer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['github.com', 'https://youtu.be/7Pq-S557XQU?t=3m32s']\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import urlextract # may require an Internet connection to download root domain names\n",
    "    \n",
    "    url_extractor = urlextract.URLExtract()\n",
    "    print(url_extractor.find_urls(\"Will it detect github.com and https://youtu.be/7Pq-S557XQU?t=3m32s\"))\n",
    "except ImportError:\n",
    "    print(\"Error: replacing URLs requires the urlextract module.\")\n",
    "    url_extractor = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class EmailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strip_headers=True, lower_case=True, remove_punctuation=True,\n",
    "                 replace_urls=True, replace_numbers=True, stemming=True):\n",
    "        self.strip_headers = strip_headers\n",
    "        self.lower_case = lower_case\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.replace_urls = replace_urls\n",
    "        self.replace_numbers = replace_numbers\n",
    "        self.stemming = stemming\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for email in X:\n",
    "            text = email_to_text(email) or \"\"\n",
    "            if self.lower_case:\n",
    "                text = text.lower()\n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*)?(?:[eE][+-]?\\d+)?', 'NUMBER', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            word_counts = Counter(text.split())\n",
    "            if self.stemming and stemmer is not None:\n",
    "                stemmed_word_counts = Counter()\n",
    "                for word, count in word_counts.items():\n",
    "                    stemmed_word = stemmer.stem(word)\n",
    "                    stemmed_word_counts[stemmed_word] += count\n",
    "                word_counts = stemmed_word_counts\n",
    "            X_transformed.append(word_counts)\n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'is': 7, 'number': 5, 's': 4, 'guarante': 4, 'a': 4, 'on': 3, 'at': 3, 'but': 3, 'i': 3, 'that': 3, 'no': 3, 'm': 3, 'the': 3, 'linux': 3, 'also': 2, 'about': 2, 'french': 2, 'consum': 2, 'law': 2, 'without': 2, 'so': 2, 'softwar': 2, 'deliv': 2, 'in': 2, 'these': 2, 'if': 2, 'thi': 2, 'not': 2, 'all': 2, 'with': 2, 'least': 2, 'which': 2, 'get': 2, 'you': 2, 'ie': 2, 'mon': 1, 'oct': 1, 'numberam': 1, 'ciaran': 1, 'johnston': 1, 'wrote': 1, 'there': 1, 'some': 1, 'stuff': 1, 'forbid': 1, 'sale': 1, 'of': 1, 'anyth': 1, 'as': 1, 'breach': 1, 'franc': 1, 'didn': 1, 't': 1, 'realli': 1, 'follow': 1, 'my': 1, 'bit': 1, 'iffi': 1, 'day': 1, 'true': 1, 'doe': 1, 'it': 1, 'nullifi': 1, 'microsoft': 1, 'adob': 1, 'and': 1, 'winzip': 1, 'licenc': 1, 'amongst': 1, 'most': 1, 'other': 1, 'claim': 1, 'liabil': 1, 'say': 1, 'fault': 1, 'they': 1, 'are': 1, 'honest': 1, 'appar': 1, 'angl': 1, 'e': 1, 'sell': 1, 'product': 1, 'sold': 1, 'servic': 1, 'licens': 1, 'what': 1, 'rememb': 1, 'read': 1, 'how': 1, 'away': 1, 'provid': 1, 'u': 1, 're': 1, 'feel': 1, 'rather': 1, 'deep': 1, 'pocket': 1, 'could': 1, 'alway': 1, 'tri': 1, 'su': 1, 'to': 1, 'court': 1, 'view': 1, 'matter': 1, 'niall': 1, 'irish': 1, 'user': 1, 'group': 1, 'ilug': 1, 'url': 1, 'for': 1, 'un': 1, 'subscript': 1, 'inform': 1, 'list': 1, 'maintain': 1, 'listmast': 1}),\n",
       "       Counter({'the': 10, 'i': 7, 'number': 6, 'glm': 5, 'a': 5, 'in': 4, 'polit': 3, 'of': 3, 'first': 3, 'amend': 3, 'that': 3, 'or': 3, 'r': 3, 'are': 2, 'which': 2, 'is': 2, 'greater': 2, 'more': 2, 'and': 2, 's': 2, 'kill': 2, 'right': 2, 'make': 2, 'forc': 2, 'but': 2, 'm': 2, 'wa': 2, 'to': 2, 'what': 2, 'be': 2, 'poll': 2, 'their': 2, 'popul': 2, 'bitbitch': 2, 'wherea': 1, 'brochur': 1, 'not': 1, 'statist': 1, 'clearli': 1, 'show': 1, 'senseless': 1, 'prevent': 1, 'killer': 1, 'so': 1, 'outsid': 1, 'occasion': 1, 'hit': 1, 'by': 1, 'mafia': 1, 'boy': 1, 'where': 1, 'if': 1, 'you': 1, 'said': 1, 'mind': 1, 'numb': 1, 'destroy': 1, 'cynic': 1, 'd': 1, 'agre': 1, 'doe': 1, 'realli': 1, 'bitter': 1, 'atm': 1, 'two': 1, 'reason': 1, 'mostli': 1, 'listen': 1, 'heard': 1, 'foundat': 1, 'on': 1, 'npr': 1, 'releas': 1, 'note': 1, 'veri': 1, 'unscientif': 1, 'found': 1, 'half': 1, 'think': 1, 'goe': 1, 'too': 1, 'far': 1, 'quot': 1, 'find': 1, 'sourc': 1, 'don': 1, 't': 1, 'even': 1, 'know': 1, 'guarante': 1, 'have': 1, 'paper': 1, 'due': 1, 'will': 1, 'break': 1, 'my': 1, 'law': 1, 'school': 1, 'career': 1, 'hate': 1, 'sheep': 1, 'baaaa': 1, 'russel': 1, 'turpin': 1, 'deafbox': 1, 'hotmail': 1, 'com': 1, 'write': 1, 'it': 1, 'difficult': 1, 'measur': 1, 'liabil': 1, 'potenti': 1, 'mate': 1, 'religios': 1, 'alcohol': 1, 'g': 1, 'best': 1, 'regard': 1, 'mailto': 1, 'magnesium': 1, 'net': 1}),\n",
       "       Counter({'the': 9, 'a': 4, 'use': 4, 'directori': 3, 'they': 3, 'is': 3, 'if': 3, 'for': 3, 'linux': 3, 'two': 2, 'same': 2, 'file': 2, 't': 2, 'tool': 2, 'are': 2, 'nice': 2, 'you': 2, 'ie': 2, 'john': 1, 'p': 1, 'looney': 1, 'wrote': 1, 'i': 1, 've': 1, 'that': 1, 'onc': 1, 'upon': 1, 'time': 1, 'contain': 1, 'now': 1, 'don': 1, 'there': 1, 'to': 1, 'merg': 1, 'creat': 1, 'new': 1, 'where': 1, 'aren': 1, 'chang': 1, 'differ': 1, 'one': 1, 'with': 1, 'most': 1, 'recent': 1, 'datestamp': 1, 'just': 1, 'record': 1, 'mc': 1, 'ha': 1, 'comparison': 1, 'function': 1, 'thi': 1, 'realli': 1, 'when': 1, 'ftp': 1, 'vf': 1, 'e': 1, 'g': 1, 'of': 1, 'cours': 1, 'someth': 1, 'like': 1, 'ftpf': 1, 'can': 1, 'previous': 1, 'mention': 1, 'pádraig': 1, 'irish': 1, 'user': 1, 'group': 1, 'ilug': 1, 'url': 1, 'un': 1, 'subscript': 1, 'inform': 1, 'list': 1, 'maintain': 1, 'listmast': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few = X_train[:3]\n",
    "X_few_wordcounts = EmailToWordCounterTransformer().fit_transform(X_few)\n",
    "X_few_wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class WordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, vocabulary_size=1000):\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "    def fit(self, X, y=None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for word, count in word_count.items():\n",
    "                total_count[word] += min(count, 10)\n",
    "        most_common = total_count.most_common()[:self.vocabulary_size]\n",
    "        self.vocabulary_ = {word: index + 1 for index, (word, count) in enumerate(most_common)}\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        rows = []\n",
    "        cols = []\n",
    "        data = []\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows.append(row)\n",
    "                cols.append(self.vocabulary_.get(word, 0))\n",
    "                data.append(count)\n",
    "        return csr_matrix((data, (rows, cols)), shape=(len(X), self.vocabulary_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer = WordCounterToVectorTransformer(vocabulary_size=10)\n",
    "X_few_vectors = vocab_transformer.fit_transform(X_few_wordcounts)\n",
    "X_few_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[141,   3,   4,   7,   5,   3,   3,   4,   2,   2,   3],\n",
       "       [148,  10,   5,   2,   6,   7,   3,   2,   4,   1,   0],\n",
       "       [ 89,   9,   4,   3,   0,   1,   1,   0,   0,   3,   3]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_few_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'a': 2,\n",
       " 'is': 3,\n",
       " 'number': 4,\n",
       " 'i': 5,\n",
       " 'that': 6,\n",
       " 's': 7,\n",
       " 'in': 8,\n",
       " 'if': 9,\n",
       " 'linux': 10}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_transformer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "preprocess_pipeline = Pipeline([\n",
    "    (\"email_to_wordcount\", EmailToWordCounterTransformer()),\n",
    "    (\"wordcount_to_vector\", WordCounterToVectorTransformer()),\n",
    "])\n",
    "\n",
    "X_train_transformed = preprocess_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.986) total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.986) total time=   0.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    1.0s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.978) total time=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda3/envs/tf2/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    1.6s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9833390553474822"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 99.01%\n",
      "Recall: 96.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda3/envs/tf2/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = preprocess_pipeline.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b463cf52ca8196c9c2a7ed6997d0666bf4884dc439b687c01213b9631ac3d5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
