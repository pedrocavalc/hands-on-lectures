{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questao1) Qual algoritmo de Regressão Linear você usaria para treinar dados com milões de características?**\n",
    "\n",
    "\n",
    "R: SGD ou GD em mini batch, pois necessitam passar pelo conjunto de treinamento menos vezes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questao2) Se os dados tiverem diferetes escalas, quais algoritmos podem sofrer com isso?**\n",
    "\n",
    "\n",
    "R: Os algoritmos de Gradient descent. Solução: Aplicar técnicas de escaloamento nos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão3) O GD pode ficar empacadoquando em mínimos locais quando treinamos um modelo de regressão logística?**\n",
    "\n",
    "\n",
    "R: Não, pois a função de custo da regressão logística é a conhecida como log loss, que é uma função convexa, logo o GD sempre acaba covergindo se utilizadas as learnig rates corretas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão 4) Todos os algoritmos de GD resultam o mesmo modelo?**\n",
    "\n",
    "R: Não, devido ao SGD utilizar sempre uma instância aleatória do treinamento po vez, ele oscila no mínimo global a cada iteração, embora ele gere um modelo muito parecido com os outros algoritmos eles não são iguais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão5) Imagine que você esta treiando um gradient descendente em batch e o erro de validação das curvas aumenta, o que é isso? Como fazer para melhorar?**\n",
    "\n",
    "R: Isso indica que o modelo esta se sobreajustando aos dados de treinameto e começando a fazer generalizações incorretas. Solução: Aumentar a complexidade do modelo para que ele se ajuste melhor aos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão6) É correto interromper o gradient em mini batch logo que o erro de validação aumenta?**\n",
    "\n",
    "R: Não, pois o gradient em mini batch oscila a taxa de validação durante seu treinameto, o correto é estimar um parâmetro de tolerâcia e se a validação contiuar apenas aumetando até o parâmetro, interromper o algoritmo e retornar ao mínimo alcançado anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão7) Qual algoritmo de GD chegará mais rápido próximo da solução final? Qual convergirá? Como fazer os outros convergirem?**\n",
    "\n",
    "R: O algoritmo que chegará mais rápido é o Stochastic Gradiet Descent(SGD). É garantida a convergêcia do GD em batch. PAra os outros convergirem é necessária que a learning rate seja apropriada por learning rate schedule. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão8) Suponha que você esta usando a regressão polinomial e as curvas de aprendizado entre treiamento e validação tem lacunas grandes o que pode estar acontecendo? Qual a maneira de ajeitar isso?**\n",
    "\n",
    "R: O modelo esta subajustando os dados e não consegue fazer generalizações. Solução: Remover outliers para diminuir o erro irredutivel utilizar um modelo com complexidade correta aos dados aplicar regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão9) Suponha que você utilize a regressão de Ridge e os erros de treinamento e validação são altos e iguais, ele tem variancia alta ou víes alto? O que fazer com o alpha?**\n",
    "\n",
    "R: O modelo esta se subajustando aos dados, portanto a variáncia do modelo esta muito alta, é necessário diminuir o alpha para dar mais liberdade ao modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão10) Por quais motivos preferir um modelo de regressão regularizado a outro?**\n",
    "\n",
    "R: \n",
    "\n",
    "a) É preferível utilizar a regressão de Ridge a de Linear normal devido a Ridge ter graus de regularização no modelo\n",
    "\n",
    "b) É preferível utilizar a de Lasso a de Ridge devido a regressão de Lasso aplicar uma feature selection colocando os pesos das caracterísitcas menos importantes próximod de zero\n",
    "\n",
    "c) É preferível utilizar a elastic net a de Lasso devido por que a de Lasso pode se comportar imprevisívelmente em algumas situações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão11) Suponha que você quer classificar fotos como externas e internas e diurnas e noturnas deve aplicar dois classificadores de regressão logística ou softmax?**\n",
    "\n",
    "R: Softmax pois ele consegue trabalhar com multiclasses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Questão12) Implemente o GD em batch para softmax com early stopping sem utilizar o Scikit-Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "\n",
    "# utilizando o sklearn apenas para ter um conjunto de dados para classificação\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris[\"data\"]\n",
    "y = iris[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_with_bias = np.c_[np.ones([len(X), 1]), X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 0.2\n",
    "validation_ratio = 0.2\n",
    "total_size = len(X_with_bias)\n",
    "\n",
    "## Getting the train test val split\n",
    "np.random.seed(2042)\n",
    "test_size = int(total_size * test_ratio)\n",
    "validation_size = int(total_size * validation_ratio)\n",
    "train_size = total_size - test_size - validation_size\n",
    "\n",
    "rnd_indices = np.random.permutation(total_size)\n",
    "\n",
    "X_train = X_with_bias[rnd_indices[:train_size]]\n",
    "y_train = y[rnd_indices[:train_size]]\n",
    "X_valid = X_with_bias[rnd_indices[train_size:-test_size]]\n",
    "y_valid = y[rnd_indices[train_size:-test_size]]\n",
    "X_test = X_with_bias[rnd_indices[-test_size:]]\n",
    "y_test = y[rnd_indices[-test_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_one_hot(y):\n",
    "    n_classes = y.max() + 1\n",
    "    m = len(y)\n",
    "    Y_one_hot = np.zeros((m, n_classes))\n",
    "    Y_one_hot[np.arange(m),y] = 1\n",
    "    return Y_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_one_hot = to_one_hot(y_train)\n",
    "Y_valid_one_hot = to_one_hot(y_valid)\n",
    "Y_test_one_hot = to_one_hot(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exps = np.exp(z)\n",
    "    exps_sums = np.sum(exps,axis = 1, keepdims= True)\n",
    "    return exps / exps_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 3)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_inputs = X_train.shape[1]\n",
    "n_outputs = len(np.unique(y_train))\n",
    "n_inputs, n_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "theta = np.random.randn(2, 1)\n",
    "\n",
    "def gradient_descent_batch(iterations, eta, X_b, y, m):\n",
    "    '''\n",
    "    Function to apply gradient descent in the data\n",
    "    '''\n",
    "    for iteration in range(iterations):\n",
    "        gradients = 2 /  m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "        theta = theta - eta * gradients\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration loss: 4.696856756766695\n",
      "iteration loss: 0.4891677227730179\n",
      "iteration loss: 0.3733427728820095\n",
      "iteration loss: 0.3085671322247952\n",
      "iteration loss: 0.26486352141715974\n",
      "iteration loss: 0.23330217730402159\n",
      "iteration loss: 0.20948190169446193\n",
      "iteration loss: 0.19088859944451353\n",
      "iteration loss: 0.17597852106554476\n",
      "iteration loss: 0.16375478903355625\n",
      "iteration loss: 0.15354726628278936\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(0, n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    y_proba = softmax(logits)\n",
    "    if iteration % 500 == 0:\n",
    "        loss = -np.mean(np.sum(Y_train_one_hot* np.log(y_proba + epsilon), axis = 1))\n",
    "        print(\"iteration loss:\", loss)\n",
    "    \n",
    "    error = y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error)\n",
    "    Theta = Theta - gradients * eta\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration loss: 9.4243378460493\n",
      "iteration loss: 0.6925120286190009\n",
      "iteration loss: 0.5667850277573632\n",
      "iteration loss: 0.5302798520219333\n",
      "iteration loss: 0.5177739278599961\n",
      "iteration loss: 0.5128676006649987\n",
      "iteration loss: 0.5105254667378212\n",
      "iteration loss: 0.5090876135311129\n",
      "iteration loss: 0.507985818732053\n",
      "iteration loss: 0.5070209208834864\n",
      "iteration loss: 0.5061218191980081\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "\n",
    "for iteration in range(0, n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    y_proba = softmax(logits)\n",
    "    if iteration % 500 == 0:\n",
    "        loss = -np.mean(np.sum(Y_train_one_hot* np.log(y_proba + epsilon), axis = 1))\n",
    "        l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "        loss = loss + alpha * l2_loss\n",
    "        print(\"iteration loss:\", loss)\n",
    "    \n",
    "    error = y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - gradients * eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = X_valid.dot(Theta)\n",
    "Y_proba = softmax(logits)\n",
    "y_predict = np.argmax(Y_proba, axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_valid)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 1.7858832528332267\n",
      "val_loss 1.5362645717088548\n",
      "train_loss: 0.5988134065814825\n",
      "val_loss 0.6294294094718186\n",
      "train_loss: 0.5725164911198969\n",
      "val_loss 0.6048997218273133\n",
      "train_loss: 0.5644232179336597\n",
      "val_loss 0.5979195509339079\n",
      "train_loss: 0.5599757666273641\n",
      "val_loss 0.5940611253024002\n",
      "train_loss: 0.5566231125853629\n",
      "val_loss 0.590970769717026\n",
      "train_loss: 0.5536942435969199\n",
      "val_loss 0.5881100423164763\n",
      "train_loss: 0.5509785323331566\n",
      "val_loss 0.585350017870104\n",
      "train_loss: 0.5484026293491222\n",
      "val_loss 0.5826668144598159\n",
      "train_loss: 0.5459382922946042\n",
      "val_loss 0.5800618570635322\n",
      "train_loss: 0.5435728698708912\n",
      "val_loss 0.5775401261711863\n"
     ]
    }
   ],
   "source": [
    "eta = 0.01\n",
    "n_iterations = 5001\n",
    "m = len(X_train)\n",
    "epsilon = 1e-7\n",
    "alpha = 0.1\n",
    "Theta = np.random.randn(n_inputs, n_outputs)\n",
    "best_loss = np.infty\n",
    "\n",
    "for iteration in range(0, n_iterations):\n",
    "    logits = X_train.dot(Theta)\n",
    "    y_proba = softmax(logits)\n",
    "    loss = -np.mean(np.sum(Y_train_one_hot* np.log(y_proba + epsilon), axis = 1))\n",
    "    l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "    train_loss = loss + alpha * l2_loss\n",
    "    error = y_proba - Y_train_one_hot\n",
    "    gradients = 1/m * X_train.T.dot(error) + np.r_[np.zeros([1, n_outputs]), alpha * Theta[1:]]\n",
    "    Theta = Theta - gradients * eta    \n",
    "    \n",
    "    logits = X_valid.dot(Theta)\n",
    "    y_proba = softmax(logits)\n",
    "    val_loss = -np.mean(np.sum(Y_valid_one_hot* np.log(y_proba + epsilon), axis = 1))\n",
    "    val_l2_loss = 1/2 * np.sum(np.square(Theta[1:]))\n",
    "    val_loss = val_loss + alpha * val_l2_loss\n",
    "\n",
    "    if iteration % 500 == 0:\n",
    "        print(\"train_loss:\",train_loss)\n",
    "        print(\"val_loss\",val_loss)\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        best_theta = Theta\n",
    "    else:\n",
    "        break\n",
    "    \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b463cf52ca8196c9c2a7ed6997d0666bf4884dc439b687c01213b9631ac3d5d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
