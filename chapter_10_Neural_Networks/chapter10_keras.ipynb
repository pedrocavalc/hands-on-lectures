{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "\n",
    "X = iris.data[:, (2,3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "y_pred = per_clf.predict([[2,.5]])\n",
    "y_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:27:05.670929: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.4.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "tf.__version__\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test)  = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coat'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names[y_train[0]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:27:07.551349: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-27 17:27:07.551918: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
      "2023-03-27 17:27:07.559905: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-03-27 17:27:07.560016: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
      "pciBusID: 0000:01:00.0 name: NVIDIA GeForce GTX 1060 6GB computeCapability: 6.1\n",
      "coreClock: 1.8475GHz coreCount: 10 deviceMemorySize: 5.93GiB deviceMemoryBandwidth: 178.99GiB/s\n",
      "2023-03-27 17:27:07.560038: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "2023-03-27 17:27:07.561660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
      "2023-03-27 17:27:07.561697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
      "2023-03-27 17:27:07.562224: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
      "2023-03-27 17:27:07.562376: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
      "2023-03-27 17:27:07.562483: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcusolver.so.10'; dlerror: libcusolver.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.2/lib64:/home/pedro/miniconda3/lib/:/home/pedro/miniconda3/lib/\n",
      "2023-03-27 17:27:07.562825: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
      "2023-03-27 17:27:07.562922: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
      "2023-03-27 17:27:07.562929: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1757] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-03-27 17:27:07.563132: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-27 17:27:07.563579: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2023-03-27 17:27:07.563591: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2023-03-27 17:27:07.563594: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      \n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[28,28]))\n",
    "model.add(keras.layers.Dense(300, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(100, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#or \n",
    "model = keras.models.Sequential([\n",
    "(keras.layers.Flatten(input_shape=[28,28])),\n",
    "(keras.layers.Dense(300, activation = 'relu')),\n",
    "(keras.layers.Dense(100, activation = 'relu')),\n",
    "(keras.layers.Dense(10, activation='softmax'))\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 266,610\n",
      "Trainable params: 266,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.utils.plot_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(784, 300)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_1 = model.layers[1]\n",
    "weigths, biases = hidden_1.get_weights()\n",
    "weigths.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compilando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:27:09.014018: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
      "2023-03-27 17:27:09.014303: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2899885000 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "1719/1719 [==============================] - 2s 908us/step - loss: 0.9741 - accuracy: 0.6854 - val_loss: 0.4929 - val_accuracy: 0.8342\n",
      "Epoch 2/30\n",
      "1719/1719 [==============================] - 1s 746us/step - loss: 0.5008 - accuracy: 0.8265 - val_loss: 0.4558 - val_accuracy: 0.8436\n",
      "Epoch 3/30\n",
      "1719/1719 [==============================] - 1s 757us/step - loss: 0.4517 - accuracy: 0.8431 - val_loss: 0.4098 - val_accuracy: 0.8604\n",
      "Epoch 4/30\n",
      "1719/1719 [==============================] - 1s 748us/step - loss: 0.4203 - accuracy: 0.8526 - val_loss: 0.4021 - val_accuracy: 0.8642\n",
      "Epoch 5/30\n",
      "1719/1719 [==============================] - 1s 761us/step - loss: 0.3945 - accuracy: 0.8615 - val_loss: 0.4077 - val_accuracy: 0.8622\n",
      "Epoch 6/30\n",
      "1719/1719 [==============================] - 1s 743us/step - loss: 0.3788 - accuracy: 0.8647 - val_loss: 0.3862 - val_accuracy: 0.8664\n",
      "Epoch 7/30\n",
      "1719/1719 [==============================] - 1s 740us/step - loss: 0.3681 - accuracy: 0.8679 - val_loss: 0.3740 - val_accuracy: 0.8716\n",
      "Epoch 8/30\n",
      "1719/1719 [==============================] - 1s 739us/step - loss: 0.3515 - accuracy: 0.8762 - val_loss: 0.3615 - val_accuracy: 0.8764\n",
      "Epoch 9/30\n",
      "1719/1719 [==============================] - 1s 737us/step - loss: 0.3432 - accuracy: 0.8779 - val_loss: 0.3704 - val_accuracy: 0.8704\n",
      "Epoch 10/30\n",
      "1719/1719 [==============================] - 1s 748us/step - loss: 0.3324 - accuracy: 0.8814 - val_loss: 0.3475 - val_accuracy: 0.8776\n",
      "Epoch 11/30\n",
      "1719/1719 [==============================] - 1s 752us/step - loss: 0.3282 - accuracy: 0.8848 - val_loss: 0.3564 - val_accuracy: 0.8730\n",
      "Epoch 12/30\n",
      "1719/1719 [==============================] - 1s 741us/step - loss: 0.3186 - accuracy: 0.8867 - val_loss: 0.3357 - val_accuracy: 0.8810\n",
      "Epoch 13/30\n",
      "1719/1719 [==============================] - 1s 741us/step - loss: 0.3069 - accuracy: 0.8902 - val_loss: 0.3285 - val_accuracy: 0.8876\n",
      "Epoch 14/30\n",
      "1719/1719 [==============================] - 1s 746us/step - loss: 0.3062 - accuracy: 0.8913 - val_loss: 0.3403 - val_accuracy: 0.8814\n",
      "Epoch 15/30\n",
      "1719/1719 [==============================] - 1s 745us/step - loss: 0.2933 - accuracy: 0.8951 - val_loss: 0.3420 - val_accuracy: 0.8780\n",
      "Epoch 16/30\n",
      "1719/1719 [==============================] - 1s 761us/step - loss: 0.2890 - accuracy: 0.8954 - val_loss: 0.3228 - val_accuracy: 0.8834\n",
      "Epoch 17/30\n",
      "1719/1719 [==============================] - 1s 753us/step - loss: 0.2877 - accuracy: 0.8967 - val_loss: 0.3214 - val_accuracy: 0.8812\n",
      "Epoch 18/30\n",
      "1719/1719 [==============================] - 1s 744us/step - loss: 0.2794 - accuracy: 0.8977 - val_loss: 0.3166 - val_accuracy: 0.8864\n",
      "Epoch 19/30\n",
      "1719/1719 [==============================] - 1s 748us/step - loss: 0.2727 - accuracy: 0.9016 - val_loss: 0.3094 - val_accuracy: 0.8930\n",
      "Epoch 20/30\n",
      "1719/1719 [==============================] - 1s 751us/step - loss: 0.2717 - accuracy: 0.9033 - val_loss: 0.3114 - val_accuracy: 0.8878\n",
      "Epoch 21/30\n",
      "1719/1719 [==============================] - 1s 747us/step - loss: 0.2616 - accuracy: 0.9053 - val_loss: 0.3108 - val_accuracy: 0.8926\n",
      "Epoch 22/30\n",
      "1719/1719 [==============================] - 1s 756us/step - loss: 0.2603 - accuracy: 0.9074 - val_loss: 0.3009 - val_accuracy: 0.8904\n",
      "Epoch 23/30\n",
      "1719/1719 [==============================] - 1s 763us/step - loss: 0.2547 - accuracy: 0.9086 - val_loss: 0.3081 - val_accuracy: 0.8896\n",
      "Epoch 24/30\n",
      "1719/1719 [==============================] - 1s 747us/step - loss: 0.2508 - accuracy: 0.9100 - val_loss: 0.3114 - val_accuracy: 0.8868\n",
      "Epoch 25/30\n",
      "1719/1719 [==============================] - 1s 751us/step - loss: 0.2478 - accuracy: 0.9120 - val_loss: 0.3089 - val_accuracy: 0.8902\n",
      "Epoch 26/30\n",
      "1719/1719 [==============================] - 1s 744us/step - loss: 0.2403 - accuracy: 0.9146 - val_loss: 0.3026 - val_accuracy: 0.8922\n",
      "Epoch 27/30\n",
      "1719/1719 [==============================] - 1s 745us/step - loss: 0.2369 - accuracy: 0.9135 - val_loss: 0.2991 - val_accuracy: 0.8938\n",
      "Epoch 28/30\n",
      "1719/1719 [==============================] - 1s 752us/step - loss: 0.2294 - accuracy: 0.9198 - val_loss: 0.3292 - val_accuracy: 0.8806\n",
      "Epoch 29/30\n",
      "1719/1719 [==============================] - 1s 766us/step - loss: 0.2325 - accuracy: 0.9165 - val_loss: 0.3051 - val_accuracy: 0.8872\n",
      "Epoch 30/30\n",
      "1719/1719 [==============================] - 1s 745us/step - loss: 0.2287 - accuracy: 0.9170 - val_loss: 0.3069 - val_accuracy: 0.8894\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, epochs=30, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8,5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 664us/step - loss: 68.5612 - accuracy: 0.8453\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[68.56117248535156, 0.845300018787384]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9, 2, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ankle boot', 'Pullover', 'Trouser'], dtype='<U11')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 618us/step - loss: 1.3253 - val_loss: 1.4649\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 440us/step - loss: 1.1018 - val_loss: 0.5858\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 443us/step - loss: 0.4838 - val_loss: 0.9849\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 433us/step - loss: 0.4395 - val_loss: 3.5759\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 441us/step - loss: 0.9948 - val_loss: 2.2893\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 427us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 440us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 441us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 436us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 442us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 450us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 444us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 435us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 438us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 439us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 423us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 445us/step - loss: nan - val_loss: nan\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation = 'relu', input_shape = X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"sgd\")\n",
    "history = model.fit(X_train, y_train,epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 270us/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[nan],\n",
       "       [nan],\n",
       "       [nan]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functional Api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = keras.layers.Input(shape= X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_layer)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([hidden2, input_layer])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_layer],outputs=[output])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5],name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([hidden2, input_A])\n",
    "output = keras.layers.Dense(1,name='output')(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B],outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_A, X_train_B = X_train[:, :5], X_train[:,2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:,2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:,2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 0s 667us/step - loss: 1.5003 - val_loss: 4.4215\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 478us/step - loss: 0.5586 - val_loss: 2.7523\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 482us/step - loss: 0.4469 - val_loss: 1.8948\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.4167 - val_loss: 1.6717\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 476us/step - loss: 0.4239 - val_loss: 0.5253\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 505us/step - loss: 0.3987 - val_loss: 0.3676\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.4008 - val_loss: 0.3679\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 480us/step - loss: 0.3846 - val_loss: 0.3752\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 486us/step - loss: 0.3783 - val_loss: 0.3627\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 484us/step - loss: 0.3901 - val_loss: 0.3659\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.3990 - val_loss: 0.3619\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 477us/step - loss: 0.3854 - val_loss: 0.3561\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 485us/step - loss: 0.3704 - val_loss: 0.3486\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.3623 - val_loss: 0.3469\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 484us/step - loss: 0.3390 - val_loss: 0.3479\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 482us/step - loss: 0.3736 - val_loss: 0.3317\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 479us/step - loss: 0.3577 - val_loss: 0.3401\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 481us/step - loss: 0.3485 - val_loss: 0.3322\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 475us/step - loss: 0.3405 - val_loss: 0.3447\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 483us/step - loss: 0.3415 - val_loss: 0.3178\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B),y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 291us/step - loss: 0.3433\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A,X_test_B),y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5],name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([hidden2, input_A])\n",
    "output = keras.layers.Dense(1,name='main_output')(concat)\n",
    "output_aux = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B],outputs=[output, output_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd',loss=['mse','mse'],loss_weights=[0.9, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 770us/step - loss: 1.4083 - main_output_loss: 1.2178 - aux_output_loss: 3.1225 - val_loss: 0.5489 - val_main_output_loss: 0.6099 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.5642 - main_output_loss: 0.5102 - aux_output_loss: 1.0507 - val_loss: 0.3983 - val_main_output_loss: 0.4425 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 538us/step - loss: 0.4958 - main_output_loss: 0.4577 - aux_output_loss: 0.8389 - val_loss: 0.3571 - val_main_output_loss: 0.3968 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.4494 - main_output_loss: 0.4180 - aux_output_loss: 0.7319 - val_loss: 0.3569 - val_main_output_loss: 0.3965 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.4490 - main_output_loss: 0.4275 - aux_output_loss: 0.6427 - val_loss: 0.3423 - val_main_output_loss: 0.3804 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.4360 - main_output_loss: 0.4150 - aux_output_loss: 0.6255 - val_loss: 0.3379 - val_main_output_loss: 0.3755 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.4233 - main_output_loss: 0.4056 - aux_output_loss: 0.5829 - val_loss: 0.3415 - val_main_output_loss: 0.3794 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.4184 - main_output_loss: 0.4009 - aux_output_loss: 0.5764 - val_loss: 0.3232 - val_main_output_loss: 0.3591 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3959 - main_output_loss: 0.3786 - aux_output_loss: 0.5511 - val_loss: 0.3760 - val_main_output_loss: 0.4178 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3928 - main_output_loss: 0.3756 - aux_output_loss: 0.5472 - val_loss: 0.3196 - val_main_output_loss: 0.3551 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 582us/step - loss: 0.3992 - main_output_loss: 0.3840 - aux_output_loss: 0.5359 - val_loss: 0.3240 - val_main_output_loss: 0.3600 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3795 - main_output_loss: 0.3654 - aux_output_loss: 0.5069 - val_loss: 0.3145 - val_main_output_loss: 0.3495 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3945 - main_output_loss: 0.3802 - aux_output_loss: 0.5224 - val_loss: 0.3336 - val_main_output_loss: 0.3707 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3939 - main_output_loss: 0.3805 - aux_output_loss: 0.5149 - val_loss: 0.3210 - val_main_output_loss: 0.3567 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.4189 - main_output_loss: 0.4091 - aux_output_loss: 0.5073 - val_loss: 0.3110 - val_main_output_loss: 0.3455 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3858 - main_output_loss: 0.3738 - aux_output_loss: 0.4939 - val_loss: 0.3127 - val_main_output_loss: 0.3474 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 553us/step - loss: 0.3779 - main_output_loss: 0.3667 - aux_output_loss: 0.4789 - val_loss: 0.3057 - val_main_output_loss: 0.3397 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3586 - main_output_loss: 0.3461 - aux_output_loss: 0.4707 - val_loss: 0.4720 - val_main_output_loss: 0.5244 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.4627 - main_output_loss: 0.4590 - aux_output_loss: 0.4961 - val_loss: 0.3056 - val_main_output_loss: 0.3396 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.3863 - main_output_loss: 0.3743 - aux_output_loss: 0.4937 - val_loss: 0.3197 - val_main_output_loss: 0.3553 - val_aux_output_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=(([X_valid_A, X_valid_B],[y_valid])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 650us/step - loss: 0.3628 - main_output_loss: 0.3528 - aux_output_loss: 0.4523\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate([X_test_A,X_test_B],[y_test,y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model_keras.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SubClassing API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideDeepModel(keras.Model):\n",
    "    def __init__(self, units=30, activation = 'relu',**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.hidden1 = keras.layers.Dense(units, activation= activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation= activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self,inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A,hidden2])\n",
    "        main_output =  self.main_output(concat) \n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        return main_output, aux_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = WideDeepModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 772us/step - loss: 1.6029 - output_1_loss: 1.3503 - output_2_loss: 3.8762 - val_loss: 8.2770 - val_output_1_loss: 9.1967 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.5593 - output_1_loss: 0.5062 - output_2_loss: 1.0376 - val_loss: 1.2913 - val_output_1_loss: 1.4348 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.7308 - output_1_loss: 0.7060 - output_2_loss: 0.9543 - val_loss: 0.4660 - val_output_1_loss: 0.5177 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 535us/step - loss: 0.4672 - output_1_loss: 0.4340 - output_2_loss: 0.7662 - val_loss: 0.3702 - val_output_1_loss: 0.4114 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 540us/step - loss: 0.4475 - output_1_loss: 0.4197 - output_2_loss: 0.6980 - val_loss: 0.3487 - val_output_1_loss: 0.3875 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.4354 - output_1_loss: 0.4112 - output_2_loss: 0.6536 - val_loss: 0.3422 - val_output_1_loss: 0.3802 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 547us/step - loss: 0.4697 - output_1_loss: 0.4466 - output_2_loss: 0.6781 - val_loss: 0.3449 - val_output_1_loss: 0.3832 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.4250 - output_1_loss: 0.4024 - output_2_loss: 0.6282 - val_loss: 0.3489 - val_output_1_loss: 0.3876 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.4189 - output_1_loss: 0.3976 - output_2_loss: 0.6108 - val_loss: 0.3393 - val_output_1_loss: 0.3770 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.4201 - output_1_loss: 0.4018 - output_2_loss: 0.5853 - val_loss: 0.3310 - val_output_1_loss: 0.3678 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.4024 - output_1_loss: 0.3830 - output_2_loss: 0.5773 - val_loss: 0.3331 - val_output_1_loss: 0.3701 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 534us/step - loss: 0.4065 - output_1_loss: 0.3857 - output_2_loss: 0.5940 - val_loss: 0.3503 - val_output_1_loss: 0.3893 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 570us/step - loss: 0.4053 - output_1_loss: 0.3870 - output_2_loss: 0.5706 - val_loss: 0.3267 - val_output_1_loss: 0.3631 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.4221 - output_1_loss: 0.4041 - output_2_loss: 0.5838 - val_loss: 0.3479 - val_output_1_loss: 0.3865 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.4126 - output_1_loss: 0.3947 - output_2_loss: 0.5741 - val_loss: 0.3237 - val_output_1_loss: 0.3596 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.4102 - output_1_loss: 0.3908 - output_2_loss: 0.5844 - val_loss: 0.3195 - val_output_1_loss: 0.3550 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 534us/step - loss: 0.3957 - output_1_loss: 0.3760 - output_2_loss: 0.5734 - val_loss: 0.3192 - val_output_1_loss: 0.3547 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3858 - output_1_loss: 0.3644 - output_2_loss: 0.5780 - val_loss: 0.3132 - val_output_1_loss: 0.3480 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 537us/step - loss: 0.3920 - output_1_loss: 0.3742 - output_2_loss: 0.5521 - val_loss: 0.3159 - val_output_1_loss: 0.3510 - val_output_2_loss: 0.0000e+00\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 534us/step - loss: 0.3954 - output_1_loss: 0.3776 - output_2_loss: 0.5555 - val_loss: 0.3108 - val_output_1_loss: 0.3453 - val_output_2_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd',loss=['mse','mse'],loss_weights=[0.9, 0.1])\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20, validation_data=(([X_valid_A, X_valid_B],[y_valid])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"my_model_keras.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5],name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([hidden2, input_A])\n",
    "output = keras.layers.Dense(1,name='main_output')(concat)\n",
    "output_aux = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B],outputs=[output, output_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "363/363 [==============================] - 1s 769us/step - loss: 1.5557 - main_output_loss: 1.4096 - aux_output_loss: 2.8714 - val_loss: 4.2672 - val_main_output_loss: 4.7414 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 2/200\n",
      "363/363 [==============================] - 0s 549us/step - loss: 0.5748 - main_output_loss: 0.5050 - aux_output_loss: 1.2026 - val_loss: 0.4930 - val_main_output_loss: 0.5478 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 3/200\n",
      "363/363 [==============================] - 0s 560us/step - loss: 0.5033 - main_output_loss: 0.4488 - aux_output_loss: 0.9939 - val_loss: 0.4376 - val_main_output_loss: 0.4862 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 4/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.4684 - main_output_loss: 0.4264 - aux_output_loss: 0.8468 - val_loss: 0.4102 - val_main_output_loss: 0.4558 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 5/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.4465 - main_output_loss: 0.4127 - aux_output_loss: 0.7502 - val_loss: 0.4024 - val_main_output_loss: 0.4471 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 6/200\n",
      "363/363 [==============================] - 0s 552us/step - loss: 0.4333 - main_output_loss: 0.4031 - aux_output_loss: 0.7051 - val_loss: 0.3821 - val_main_output_loss: 0.4245 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 7/200\n",
      "363/363 [==============================] - 0s 549us/step - loss: 0.4166 - main_output_loss: 0.3894 - aux_output_loss: 0.6608 - val_loss: 0.3643 - val_main_output_loss: 0.4048 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 8/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.4155 - main_output_loss: 0.3930 - aux_output_loss: 0.6178 - val_loss: 0.3730 - val_main_output_loss: 0.4144 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 9/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.4328 - main_output_loss: 0.4120 - aux_output_loss: 0.6194 - val_loss: 0.3389 - val_main_output_loss: 0.3766 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 10/200\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.3938 - main_output_loss: 0.3749 - aux_output_loss: 0.5638 - val_loss: 0.3449 - val_main_output_loss: 0.3832 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 11/200\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3979 - main_output_loss: 0.3799 - aux_output_loss: 0.5602 - val_loss: 0.3288 - val_main_output_loss: 0.3654 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 12/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.3870 - main_output_loss: 0.3698 - aux_output_loss: 0.5413 - val_loss: 0.3556 - val_main_output_loss: 0.3951 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 13/200\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3803 - main_output_loss: 0.3634 - aux_output_loss: 0.5327 - val_loss: 0.3125 - val_main_output_loss: 0.3472 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 14/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3843 - main_output_loss: 0.3690 - aux_output_loss: 0.5223 - val_loss: 0.3292 - val_main_output_loss: 0.3658 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 15/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3909 - main_output_loss: 0.3765 - aux_output_loss: 0.5201 - val_loss: 0.3497 - val_main_output_loss: 0.3886 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 16/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3719 - main_output_loss: 0.3583 - aux_output_loss: 0.4945 - val_loss: 0.2990 - val_main_output_loss: 0.3322 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 17/200\n",
      "363/363 [==============================] - 0s 552us/step - loss: 0.3647 - main_output_loss: 0.3516 - aux_output_loss: 0.4825 - val_loss: 0.3013 - val_main_output_loss: 0.3348 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 18/200\n",
      "363/363 [==============================] - 0s 592us/step - loss: 0.3598 - main_output_loss: 0.3466 - aux_output_loss: 0.4781 - val_loss: 0.2903 - val_main_output_loss: 0.3226 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 19/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3721 - main_output_loss: 0.3595 - aux_output_loss: 0.4857 - val_loss: 0.2995 - val_main_output_loss: 0.3328 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 20/200\n",
      "363/363 [==============================] - 0s 540us/step - loss: 0.3619 - main_output_loss: 0.3504 - aux_output_loss: 0.4652 - val_loss: 0.2939 - val_main_output_loss: 0.3265 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 21/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3600 - main_output_loss: 0.3482 - aux_output_loss: 0.4670 - val_loss: 0.2945 - val_main_output_loss: 0.3272 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 22/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3464 - main_output_loss: 0.3350 - aux_output_loss: 0.4489 - val_loss: 0.2864 - val_main_output_loss: 0.3183 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 23/200\n",
      "363/363 [==============================] - 0s 553us/step - loss: 0.3353 - main_output_loss: 0.3242 - aux_output_loss: 0.4351 - val_loss: 0.2941 - val_main_output_loss: 0.3268 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 24/200\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3524 - main_output_loss: 0.3419 - aux_output_loss: 0.4474 - val_loss: 0.3490 - val_main_output_loss: 0.3878 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 25/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3603 - main_output_loss: 0.3494 - aux_output_loss: 0.4580 - val_loss: 0.2847 - val_main_output_loss: 0.3164 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 26/200\n",
      "363/363 [==============================] - 0s 553us/step - loss: 0.3461 - main_output_loss: 0.3358 - aux_output_loss: 0.4396 - val_loss: 0.2823 - val_main_output_loss: 0.3137 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 27/200\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3406 - main_output_loss: 0.3303 - aux_output_loss: 0.4333 - val_loss: 0.3010 - val_main_output_loss: 0.3344 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 28/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3577 - main_output_loss: 0.3481 - aux_output_loss: 0.4444 - val_loss: 0.3108 - val_main_output_loss: 0.3454 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 29/200\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.3504 - main_output_loss: 0.3417 - aux_output_loss: 0.4284 - val_loss: 0.2816 - val_main_output_loss: 0.3129 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 30/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3377 - main_output_loss: 0.3272 - aux_output_loss: 0.4317 - val_loss: 0.2773 - val_main_output_loss: 0.3081 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 31/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3506 - main_output_loss: 0.3412 - aux_output_loss: 0.4357 - val_loss: 0.2955 - val_main_output_loss: 0.3284 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 32/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3409 - main_output_loss: 0.3319 - aux_output_loss: 0.4228 - val_loss: 0.2768 - val_main_output_loss: 0.3076 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 33/200\n",
      "363/363 [==============================] - 0s 588us/step - loss: 0.3311 - main_output_loss: 0.3221 - aux_output_loss: 0.4120 - val_loss: 0.2992 - val_main_output_loss: 0.3324 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 34/200\n",
      "363/363 [==============================] - 0s 540us/step - loss: 0.3344 - main_output_loss: 0.3257 - aux_output_loss: 0.4130 - val_loss: 0.2845 - val_main_output_loss: 0.3161 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 35/200\n",
      "363/363 [==============================] - 0s 540us/step - loss: 0.3259 - main_output_loss: 0.3168 - aux_output_loss: 0.4077 - val_loss: 0.3159 - val_main_output_loss: 0.3510 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 36/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3415 - main_output_loss: 0.3325 - aux_output_loss: 0.4226 - val_loss: 0.2869 - val_main_output_loss: 0.3188 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 37/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.3391 - main_output_loss: 0.3307 - aux_output_loss: 0.4153 - val_loss: 0.3065 - val_main_output_loss: 0.3405 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 38/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3395 - main_output_loss: 0.3313 - aux_output_loss: 0.4133 - val_loss: 0.2716 - val_main_output_loss: 0.3018 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 39/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.3316 - main_output_loss: 0.3225 - aux_output_loss: 0.4140 - val_loss: 0.2921 - val_main_output_loss: 0.3246 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 40/200\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3277 - main_output_loss: 0.3186 - aux_output_loss: 0.4097 - val_loss: 0.3016 - val_main_output_loss: 0.3351 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 41/200\n",
      "363/363 [==============================] - 0s 553us/step - loss: 0.3313 - main_output_loss: 0.3221 - aux_output_loss: 0.4134 - val_loss: 0.2978 - val_main_output_loss: 0.3309 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 42/200\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3300 - main_output_loss: 0.3213 - aux_output_loss: 0.4079 - val_loss: 0.2834 - val_main_output_loss: 0.3149 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 43/200\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3430 - main_output_loss: 0.3341 - aux_output_loss: 0.4230 - val_loss: 0.3311 - val_main_output_loss: 0.3679 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 44/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3400 - main_output_loss: 0.3324 - aux_output_loss: 0.4083 - val_loss: 0.2796 - val_main_output_loss: 0.3107 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 45/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3209 - main_output_loss: 0.3130 - aux_output_loss: 0.3914 - val_loss: 0.3135 - val_main_output_loss: 0.3483 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 46/200\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3192 - main_output_loss: 0.3111 - aux_output_loss: 0.3917 - val_loss: 0.2678 - val_main_output_loss: 0.2976 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 47/200\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3220 - main_output_loss: 0.3140 - aux_output_loss: 0.3947 - val_loss: 0.3148 - val_main_output_loss: 0.3498 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 48/200\n",
      "363/363 [==============================] - 0s 533us/step - loss: 0.3281 - main_output_loss: 0.3200 - aux_output_loss: 0.4010 - val_loss: 0.2800 - val_main_output_loss: 0.3111 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 49/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.3286 - main_output_loss: 0.3209 - aux_output_loss: 0.3973 - val_loss: 0.2685 - val_main_output_loss: 0.2983 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 50/200\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.3390 - main_output_loss: 0.3304 - aux_output_loss: 0.4161 - val_loss: 0.2965 - val_main_output_loss: 0.3295 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 51/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3417 - main_output_loss: 0.3338 - aux_output_loss: 0.4128 - val_loss: 0.2866 - val_main_output_loss: 0.3184 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 52/200\n",
      "363/363 [==============================] - 0s 538us/step - loss: 0.3218 - main_output_loss: 0.3134 - aux_output_loss: 0.3972 - val_loss: 0.2832 - val_main_output_loss: 0.3147 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 53/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3331 - main_output_loss: 0.3255 - aux_output_loss: 0.4016 - val_loss: 0.2743 - val_main_output_loss: 0.3047 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 54/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3378 - main_output_loss: 0.3300 - aux_output_loss: 0.4074 - val_loss: 0.3447 - val_main_output_loss: 0.3830 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 55/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3185 - main_output_loss: 0.3100 - aux_output_loss: 0.3947 - val_loss: 0.2675 - val_main_output_loss: 0.2972 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 56/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3338 - main_output_loss: 0.3266 - aux_output_loss: 0.3986 - val_loss: 0.3374 - val_main_output_loss: 0.3748 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 57/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.3120 - main_output_loss: 0.3046 - aux_output_loss: 0.3788 - val_loss: 0.2833 - val_main_output_loss: 0.3147 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 58/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3066 - main_output_loss: 0.2990 - aux_output_loss: 0.3752 - val_loss: 0.4349 - val_main_output_loss: 0.4832 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 59/200\n",
      "363/363 [==============================] - 0s 539us/step - loss: 0.3336 - main_output_loss: 0.3255 - aux_output_loss: 0.4063 - val_loss: 0.2710 - val_main_output_loss: 0.3011 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 60/200\n",
      "363/363 [==============================] - 0s 545us/step - loss: 0.3241 - main_output_loss: 0.3159 - aux_output_loss: 0.3985 - val_loss: 0.3762 - val_main_output_loss: 0.4180 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 61/200\n",
      "363/363 [==============================] - 0s 546us/step - loss: 0.3207 - main_output_loss: 0.3133 - aux_output_loss: 0.3865 - val_loss: 0.2754 - val_main_output_loss: 0.3059 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 62/200\n",
      "363/363 [==============================] - 0s 536us/step - loss: 0.3231 - main_output_loss: 0.3160 - aux_output_loss: 0.3867 - val_loss: 0.4340 - val_main_output_loss: 0.4823 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 63/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3127 - main_output_loss: 0.3049 - aux_output_loss: 0.3832 - val_loss: 0.2821 - val_main_output_loss: 0.3134 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 64/200\n",
      "363/363 [==============================] - 0s 541us/step - loss: 0.3283 - main_output_loss: 0.3207 - aux_output_loss: 0.3961 - val_loss: 0.5646 - val_main_output_loss: 0.6273 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 65/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3250 - main_output_loss: 0.3176 - aux_output_loss: 0.3920 - val_loss: 0.3248 - val_main_output_loss: 0.3609 - val_aux_output_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.compile(optimizer='sgd',loss=['mse','mse'],loss_weights=[0.9, 0.1])\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=200, validation_data=(([X_valid_A, X_valid_B],[y_valid])),callbacks=[checkpoint_cb,early_stopping_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class PrintValTrainRatioCallback(keras.callbacks.Callback):\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        print(\"\\nval/train: {:.2f}\".format(logs[\"val_loss\"]/ logs[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:28:22.506257: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2023-03-27 17:28:22.506288: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2023-03-27 17:28:22.506318: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1365] Profiler found 1 GPUs\n",
      "2023-03-27 17:28:22.506516: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcupti.so.11.0'; dlerror: libcupti.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.2/lib64:/home/pedro/miniconda3/lib/:/home/pedro/miniconda3/lib/\n",
      "2023-03-27 17:28:22.507129: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcupti.so\n",
      "2023-03-27 17:28:22.514397: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2023-03-27 17:28:22.514507: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "root_log_dir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_log_dir():\n",
    "    import time\n",
    "    run_id = time.strftime(\"run_%Y_%m_%d-%H_%M_%S\")\n",
    "    return os.path.join(root_log_dir, run_id)\n",
    "\n",
    "run_log_dir = get_run_log_dir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_log_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5],name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6],name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30,activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([hidden2, input_A])\n",
    "output = keras.layers.Dense(1,name='main_output')(concat)\n",
    "output_aux = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B],outputs=[output, output_aux])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "316/363 [=========================>....] - ETA: 0s - loss: 1.6665 - main_output_loss: 1.5652 - aux_output_loss: 2.5783"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 17:28:23.068506: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2023-03-27 17:28:23.068526: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2023-03-27 17:28:23.076722: I tensorflow/core/profiler/lib/profiler_session.cc:71] Profiler session collecting data.\n",
      "2023-03-27 17:28:23.076798: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n",
      "2023-03-27 17:28:23.078799: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:228]  GpuTracer has collected 0 callback api events and 0 activity events. \n",
      "2023-03-27 17:28:23.079238: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2023-03-27 17:28:23.080286: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23\n",
      "2023-03-27 17:28:23.080842: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for trace.json.gz to ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23/pedro.trace.json.gz\n",
      "2023-03-27 17:28:23.082350: I tensorflow/core/profiler/rpc/client/save_profile.cc:137] Creating directory: ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23\n",
      "2023-03-27 17:28:23.082396: I tensorflow/core/profiler/rpc/client/save_profile.cc:143] Dumped gzipped tool data for memory_profile.json.gz to ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23/pedro.memory_profile.json.gz\n",
      "2023-03-27 17:28:23.082531: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23Dumped tool data for xplane.pb to ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23/pedro.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23/pedro.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23/pedro.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23/pedro.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./my_logs/run_2023_03_27-17_28_22/train/plugins/profile/2023_03_27_17_28_23/pedro.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363/363 [==============================] - 1s 849us/step - loss: 1.5745 - main_output_loss: 1.4752 - aux_output_loss: 2.4680 - val_loss: 1.5976 - val_main_output_loss: 1.7751 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 2/200\n",
      "363/363 [==============================] - 0s 601us/step - loss: 0.5835 - main_output_loss: 0.5218 - aux_output_loss: 1.1392 - val_loss: 2.2109 - val_main_output_loss: 2.4566 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 3/200\n",
      "363/363 [==============================] - 0s 558us/step - loss: 0.5502 - main_output_loss: 0.5043 - aux_output_loss: 0.9631 - val_loss: 0.6184 - val_main_output_loss: 0.6871 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 4/200\n",
      "363/363 [==============================] - 0s 581us/step - loss: 0.4806 - main_output_loss: 0.4402 - aux_output_loss: 0.8441 - val_loss: 0.3841 - val_main_output_loss: 0.4268 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 5/200\n",
      "363/363 [==============================] - 0s 563us/step - loss: 0.4447 - main_output_loss: 0.4149 - aux_output_loss: 0.7131 - val_loss: 0.3793 - val_main_output_loss: 0.4215 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 6/200\n",
      "363/363 [==============================] - 0s 576us/step - loss: 0.4270 - main_output_loss: 0.4022 - aux_output_loss: 0.6509 - val_loss: 0.3789 - val_main_output_loss: 0.4210 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 7/200\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.4353 - main_output_loss: 0.4120 - aux_output_loss: 0.6452 - val_loss: 0.4129 - val_main_output_loss: 0.4588 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 8/200\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.4126 - main_output_loss: 0.3926 - aux_output_loss: 0.5927 - val_loss: 0.3551 - val_main_output_loss: 0.3946 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 9/200\n",
      "363/363 [==============================] - 0s 577us/step - loss: 0.3943 - main_output_loss: 0.3756 - aux_output_loss: 0.5623 - val_loss: 0.3731 - val_main_output_loss: 0.4145 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 10/200\n",
      "363/363 [==============================] - 0s 569us/step - loss: 0.4377 - main_output_loss: 0.4218 - aux_output_loss: 0.5806 - val_loss: 0.3562 - val_main_output_loss: 0.3958 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 11/200\n",
      "363/363 [==============================] - 0s 568us/step - loss: 0.3964 - main_output_loss: 0.3791 - aux_output_loss: 0.5519 - val_loss: 0.3400 - val_main_output_loss: 0.3778 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 12/200\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.4039 - main_output_loss: 0.3870 - aux_output_loss: 0.5561 - val_loss: 0.3745 - val_main_output_loss: 0.4161 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 13/200\n",
      "363/363 [==============================] - 0s 558us/step - loss: 0.4111 - main_output_loss: 0.3950 - aux_output_loss: 0.5559 - val_loss: 0.3254 - val_main_output_loss: 0.3615 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 14/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3834 - main_output_loss: 0.3670 - aux_output_loss: 0.5315 - val_loss: 0.3420 - val_main_output_loss: 0.3800 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 15/200\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.3814 - main_output_loss: 0.3654 - aux_output_loss: 0.5255 - val_loss: 0.3449 - val_main_output_loss: 0.3833 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 16/200\n",
      "363/363 [==============================] - 0s 572us/step - loss: 0.3736 - main_output_loss: 0.3583 - aux_output_loss: 0.5112 - val_loss: 0.3241 - val_main_output_loss: 0.3601 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 17/200\n",
      "363/363 [==============================] - 0s 564us/step - loss: 0.3743 - main_output_loss: 0.3591 - aux_output_loss: 0.5111 - val_loss: 0.3425 - val_main_output_loss: 0.3805 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 18/200\n",
      "363/363 [==============================] - 0s 561us/step - loss: 0.3738 - main_output_loss: 0.3581 - aux_output_loss: 0.5153 - val_loss: 0.3671 - val_main_output_loss: 0.4079 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 19/200\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3723 - main_output_loss: 0.3574 - aux_output_loss: 0.5066 - val_loss: 0.3023 - val_main_output_loss: 0.3358 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 20/200\n",
      "363/363 [==============================] - 0s 556us/step - loss: 0.3734 - main_output_loss: 0.3586 - aux_output_loss: 0.5058 - val_loss: 0.3556 - val_main_output_loss: 0.3951 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 21/200\n",
      "363/363 [==============================] - 0s 562us/step - loss: 0.3705 - main_output_loss: 0.3559 - aux_output_loss: 0.5019 - val_loss: 0.3360 - val_main_output_loss: 0.3733 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 22/200\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.3682 - main_output_loss: 0.3548 - aux_output_loss: 0.4886 - val_loss: 0.3351 - val_main_output_loss: 0.3723 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 23/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3708 - main_output_loss: 0.3583 - aux_output_loss: 0.4827 - val_loss: 0.3213 - val_main_output_loss: 0.3570 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 24/200\n",
      "363/363 [==============================] - 0s 558us/step - loss: 0.3691 - main_output_loss: 0.3562 - aux_output_loss: 0.4854 - val_loss: 0.3397 - val_main_output_loss: 0.3774 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 25/200\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.3675 - main_output_loss: 0.3554 - aux_output_loss: 0.4766 - val_loss: 0.2901 - val_main_output_loss: 0.3224 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 26/200\n",
      "363/363 [==============================] - 0s 556us/step - loss: 0.3655 - main_output_loss: 0.3534 - aux_output_loss: 0.4749 - val_loss: 0.3924 - val_main_output_loss: 0.4360 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 27/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3776 - main_output_loss: 0.3658 - aux_output_loss: 0.4842 - val_loss: 0.4218 - val_main_output_loss: 0.4687 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 28/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3641 - main_output_loss: 0.3516 - aux_output_loss: 0.4763 - val_loss: 0.2920 - val_main_output_loss: 0.3244 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 29/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3555 - main_output_loss: 0.3429 - aux_output_loss: 0.4682 - val_loss: 0.3934 - val_main_output_loss: 0.4371 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 30/200\n",
      "363/363 [==============================] - 0s 559us/step - loss: 0.3526 - main_output_loss: 0.3395 - aux_output_loss: 0.4706 - val_loss: 0.2869 - val_main_output_loss: 0.3187 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 31/200\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3430 - main_output_loss: 0.3313 - aux_output_loss: 0.4478 - val_loss: 0.3781 - val_main_output_loss: 0.4201 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 32/200\n",
      "363/363 [==============================] - 0s 565us/step - loss: 0.3421 - main_output_loss: 0.3309 - aux_output_loss: 0.4432 - val_loss: 0.3271 - val_main_output_loss: 0.3635 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 33/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3538 - main_output_loss: 0.3433 - aux_output_loss: 0.4477 - val_loss: 0.3146 - val_main_output_loss: 0.3495 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 34/200\n",
      "363/363 [==============================] - 0s 552us/step - loss: 0.3405 - main_output_loss: 0.3295 - aux_output_loss: 0.4399 - val_loss: 0.2901 - val_main_output_loss: 0.3224 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 35/200\n",
      "363/363 [==============================] - 0s 587us/step - loss: 0.3439 - main_output_loss: 0.3326 - aux_output_loss: 0.4461 - val_loss: 0.4746 - val_main_output_loss: 0.5273 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 36/200\n",
      "363/363 [==============================] - 0s 616us/step - loss: 0.3320 - main_output_loss: 0.3211 - aux_output_loss: 0.4302 - val_loss: 0.3201 - val_main_output_loss: 0.3556 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 37/200\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3427 - main_output_loss: 0.3315 - aux_output_loss: 0.4441 - val_loss: 0.3348 - val_main_output_loss: 0.3720 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 38/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3404 - main_output_loss: 0.3290 - aux_output_loss: 0.4434 - val_loss: 0.3303 - val_main_output_loss: 0.3670 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 39/200\n",
      "363/363 [==============================] - 0s 573us/step - loss: 0.3344 - main_output_loss: 0.3235 - aux_output_loss: 0.4324 - val_loss: 0.3601 - val_main_output_loss: 0.4001 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 40/200\n",
      "363/363 [==============================] - 0s 589us/step - loss: 0.3406 - main_output_loss: 0.3289 - aux_output_loss: 0.4461 - val_loss: 0.2811 - val_main_output_loss: 0.3124 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 41/200\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3422 - main_output_loss: 0.3325 - aux_output_loss: 0.4291 - val_loss: 0.5331 - val_main_output_loss: 0.5923 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 42/200\n",
      "363/363 [==============================] - 0s 560us/step - loss: 0.3345 - main_output_loss: 0.3243 - aux_output_loss: 0.4264 - val_loss: 0.2829 - val_main_output_loss: 0.3143 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 43/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3299 - main_output_loss: 0.3199 - aux_output_loss: 0.4201 - val_loss: 0.4445 - val_main_output_loss: 0.4938 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 44/200\n",
      "363/363 [==============================] - 0s 549us/step - loss: 0.3284 - main_output_loss: 0.3188 - aux_output_loss: 0.4146 - val_loss: 0.3197 - val_main_output_loss: 0.3553 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 45/200\n",
      "363/363 [==============================] - 0s 544us/step - loss: 0.3380 - main_output_loss: 0.3283 - aux_output_loss: 0.4257 - val_loss: 0.3229 - val_main_output_loss: 0.3588 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 46/200\n",
      "363/363 [==============================] - 0s 552us/step - loss: 0.3411 - main_output_loss: 0.3314 - aux_output_loss: 0.4278 - val_loss: 0.2975 - val_main_output_loss: 0.3306 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 47/200\n",
      "363/363 [==============================] - 0s 560us/step - loss: 0.3442 - main_output_loss: 0.3350 - aux_output_loss: 0.4271 - val_loss: 0.2798 - val_main_output_loss: 0.3108 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 48/200\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.3226 - main_output_loss: 0.3120 - aux_output_loss: 0.4175 - val_loss: 0.3670 - val_main_output_loss: 0.4078 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 49/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3312 - main_output_loss: 0.3223 - aux_output_loss: 0.4110 - val_loss: 0.3604 - val_main_output_loss: 0.4005 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 50/200\n",
      "363/363 [==============================] - 0s 566us/step - loss: 0.3326 - main_output_loss: 0.3230 - aux_output_loss: 0.4187 - val_loss: 0.2794 - val_main_output_loss: 0.3105 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 51/200\n",
      "363/363 [==============================] - 0s 558us/step - loss: 0.3289 - main_output_loss: 0.3195 - aux_output_loss: 0.4130 - val_loss: 0.3648 - val_main_output_loss: 0.4053 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 52/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3359 - main_output_loss: 0.3261 - aux_output_loss: 0.4242 - val_loss: 0.2834 - val_main_output_loss: 0.3149 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 53/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3422 - main_output_loss: 0.3339 - aux_output_loss: 0.4166 - val_loss: 0.3330 - val_main_output_loss: 0.3700 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 54/200\n",
      "363/363 [==============================] - 0s 549us/step - loss: 0.3449 - main_output_loss: 0.3375 - aux_output_loss: 0.4121 - val_loss: 0.2807 - val_main_output_loss: 0.3119 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 55/200\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.3083 - main_output_loss: 0.2986 - aux_output_loss: 0.3955 - val_loss: 0.3293 - val_main_output_loss: 0.3659 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 56/200\n",
      "363/363 [==============================] - 0s 551us/step - loss: 0.3230 - main_output_loss: 0.3140 - aux_output_loss: 0.4038 - val_loss: 0.3700 - val_main_output_loss: 0.4111 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 57/200\n",
      "363/363 [==============================] - 0s 542us/step - loss: 0.3264 - main_output_loss: 0.3188 - aux_output_loss: 0.3952 - val_loss: 0.3246 - val_main_output_loss: 0.3607 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 58/200\n",
      "363/363 [==============================] - 0s 553us/step - loss: 0.3389 - main_output_loss: 0.3305 - aux_output_loss: 0.4149 - val_loss: 0.2765 - val_main_output_loss: 0.3072 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 59/200\n",
      "363/363 [==============================] - 0s 554us/step - loss: 0.3398 - main_output_loss: 0.3312 - aux_output_loss: 0.4180 - val_loss: 0.3092 - val_main_output_loss: 0.3435 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 60/200\n",
      "363/363 [==============================] - 0s 557us/step - loss: 0.3325 - main_output_loss: 0.3241 - aux_output_loss: 0.4083 - val_loss: 0.3194 - val_main_output_loss: 0.3549 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 61/200\n",
      "363/363 [==============================] - 0s 558us/step - loss: 0.3176 - main_output_loss: 0.3088 - aux_output_loss: 0.3971 - val_loss: 0.3595 - val_main_output_loss: 0.3995 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 62/200\n",
      "363/363 [==============================] - 0s 553us/step - loss: 0.3344 - main_output_loss: 0.3253 - aux_output_loss: 0.4162 - val_loss: 0.2735 - val_main_output_loss: 0.3039 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 63/200\n",
      "363/363 [==============================] - 0s 550us/step - loss: 0.3158 - main_output_loss: 0.3073 - aux_output_loss: 0.3926 - val_loss: 0.3047 - val_main_output_loss: 0.3385 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 64/200\n",
      "363/363 [==============================] - 0s 575us/step - loss: 0.3211 - main_output_loss: 0.3133 - aux_output_loss: 0.3912 - val_loss: 0.3210 - val_main_output_loss: 0.3567 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 65/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3196 - main_output_loss: 0.3115 - aux_output_loss: 0.3925 - val_loss: 0.3444 - val_main_output_loss: 0.3826 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 66/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3266 - main_output_loss: 0.3180 - aux_output_loss: 0.4035 - val_loss: 0.2953 - val_main_output_loss: 0.3281 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 67/200\n",
      "363/363 [==============================] - 0s 543us/step - loss: 0.3113 - main_output_loss: 0.3026 - aux_output_loss: 0.3895 - val_loss: 0.3119 - val_main_output_loss: 0.3466 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 68/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3065 - main_output_loss: 0.2979 - aux_output_loss: 0.3837 - val_loss: 0.3460 - val_main_output_loss: 0.3844 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 69/200\n",
      "363/363 [==============================] - 0s 555us/step - loss: 0.3149 - main_output_loss: 0.3061 - aux_output_loss: 0.3948 - val_loss: 0.2879 - val_main_output_loss: 0.3199 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 70/200\n",
      "363/363 [==============================] - 0s 591us/step - loss: 0.3265 - main_output_loss: 0.3176 - aux_output_loss: 0.4062 - val_loss: 0.2888 - val_main_output_loss: 0.3209 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 71/200\n",
      "363/363 [==============================] - 0s 556us/step - loss: 0.3293 - main_output_loss: 0.3211 - aux_output_loss: 0.4027 - val_loss: 0.2814 - val_main_output_loss: 0.3126 - val_aux_output_loss: 0.0000e+00\n",
      "Epoch 72/200\n",
      "363/363 [==============================] - 0s 548us/step - loss: 0.3321 - main_output_loss: 0.3263 - aux_output_loss: 0.3842 - val_loss: 0.3335 - val_main_output_loss: 0.3705 - val_aux_output_loss: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "model.compile(optimizer='sgd',loss=['mse','mse'],loss_weights=[0.9, 0.1])\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=200, validation_data=(([X_valid_A, X_valid_B],[y_valid])),callbacks=[checkpoint_cb,early_stopping_callback,tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hiperparametros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden = 1, n_neurons = 30, learning_rate = 3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation='relu'))\n",
    "    \n",
    "    model.add(keras.layers.Dense(1))\n",
    "    optimizer = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model.compile(loss='mse', optimizer= optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "363/363 [==============================] - 0s 641us/step - loss: 2.0200 - val_loss: 20.3059\n",
      "Epoch 2/100\n",
      "363/363 [==============================] - 0s 453us/step - loss: 0.9397 - val_loss: 23.1636\n",
      "Epoch 3/100\n",
      "363/363 [==============================] - 0s 482us/step - loss: 0.6763 - val_loss: 0.6091\n",
      "Epoch 4/100\n",
      "363/363 [==============================] - 0s 454us/step - loss: 0.5268 - val_loss: 0.4650\n",
      "Epoch 5/100\n",
      "363/363 [==============================] - 0s 447us/step - loss: 0.4744 - val_loss: 0.4266\n",
      "Epoch 6/100\n",
      "363/363 [==============================] - 0s 443us/step - loss: 0.4687 - val_loss: 0.4166\n",
      "Epoch 7/100\n",
      "363/363 [==============================] - 0s 452us/step - loss: 0.4381 - val_loss: 0.4089\n",
      "Epoch 8/100\n",
      "363/363 [==============================] - 0s 457us/step - loss: 0.4374 - val_loss: 0.4157\n",
      "Epoch 9/100\n",
      "363/363 [==============================] - 0s 453us/step - loss: 0.4231 - val_loss: 0.4022\n",
      "Epoch 10/100\n",
      "363/363 [==============================] - 0s 451us/step - loss: 0.4320 - val_loss: 0.4032\n",
      "Epoch 11/100\n",
      "363/363 [==============================] - 0s 448us/step - loss: 0.4347 - val_loss: 0.3959\n",
      "Epoch 12/100\n",
      "363/363 [==============================] - 0s 445us/step - loss: 0.4128 - val_loss: 0.3965\n",
      "Epoch 13/100\n",
      "363/363 [==============================] - 0s 446us/step - loss: 0.4017 - val_loss: 0.3967\n",
      "Epoch 14/100\n",
      "363/363 [==============================] - 0s 457us/step - loss: 0.4047 - val_loss: 0.3945\n",
      "Epoch 15/100\n",
      "363/363 [==============================] - 0s 459us/step - loss: 0.4075 - val_loss: 0.3937\n",
      "Epoch 16/100\n",
      "363/363 [==============================] - 0s 450us/step - loss: 0.4173 - val_loss: 0.3843\n",
      "Epoch 17/100\n",
      "363/363 [==============================] - 0s 454us/step - loss: 0.3905 - val_loss: 0.3857\n",
      "Epoch 18/100\n",
      "363/363 [==============================] - 0s 448us/step - loss: 0.4140 - val_loss: 0.3834\n",
      "Epoch 19/100\n",
      "363/363 [==============================] - 0s 443us/step - loss: 0.3908 - val_loss: 0.3701\n",
      "Epoch 20/100\n",
      "363/363 [==============================] - 0s 450us/step - loss: 0.4004 - val_loss: 0.3649\n",
      "Epoch 21/100\n",
      "363/363 [==============================] - 0s 450us/step - loss: 0.3825 - val_loss: 0.3728\n",
      "Epoch 22/100\n",
      "363/363 [==============================] - 0s 460us/step - loss: 0.3907 - val_loss: 0.3738\n",
      "Epoch 23/100\n",
      "363/363 [==============================] - 0s 449us/step - loss: 0.3792 - val_loss: 0.3800\n",
      "Epoch 24/100\n",
      "363/363 [==============================] - 0s 460us/step - loss: 0.3918 - val_loss: 0.3646\n",
      "Epoch 25/100\n",
      "363/363 [==============================] - 0s 447us/step - loss: 0.3778 - val_loss: 0.3642\n",
      "Epoch 26/100\n",
      "363/363 [==============================] - 0s 453us/step - loss: 0.3970 - val_loss: 0.3820\n",
      "Epoch 27/100\n",
      "363/363 [==============================] - 0s 460us/step - loss: 0.3886 - val_loss: 0.3857\n",
      "Epoch 28/100\n",
      "363/363 [==============================] - 0s 455us/step - loss: 0.3769 - val_loss: 0.3857\n",
      "Epoch 29/100\n",
      "363/363 [==============================] - 0s 449us/step - loss: 0.3747 - val_loss: 0.3791\n",
      "Epoch 30/100\n",
      "363/363 [==============================] - 0s 450us/step - loss: 0.3738 - val_loss: 0.4039\n",
      "Epoch 31/100\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.3791 - val_loss: 0.3713\n",
      "Epoch 32/100\n",
      "363/363 [==============================] - 0s 452us/step - loss: 0.3600 - val_loss: 0.3773\n",
      "Epoch 33/100\n",
      "363/363 [==============================] - 0s 463us/step - loss: 0.3809 - val_loss: 0.3533\n",
      "Epoch 34/100\n",
      "363/363 [==============================] - 0s 450us/step - loss: 0.3723 - val_loss: 0.3484\n",
      "Epoch 35/100\n",
      "363/363 [==============================] - 0s 457us/step - loss: 0.3754 - val_loss: 0.3528\n",
      "Epoch 36/100\n",
      "363/363 [==============================] - 0s 449us/step - loss: 0.3682 - val_loss: 0.3536\n",
      "Epoch 37/100\n",
      "363/363 [==============================] - 0s 452us/step - loss: 0.3629 - val_loss: 0.3632\n",
      "Epoch 38/100\n",
      "363/363 [==============================] - 0s 452us/step - loss: 0.3671 - val_loss: 0.3701\n",
      "Epoch 39/100\n",
      "363/363 [==============================] - 0s 448us/step - loss: 0.3672 - val_loss: 0.3765\n",
      "Epoch 40/100\n",
      "363/363 [==============================] - 0s 492us/step - loss: 0.3588 - val_loss: 0.3557\n",
      "Epoch 41/100\n",
      "363/363 [==============================] - 0s 462us/step - loss: 0.3484 - val_loss: 0.3413\n",
      "Epoch 42/100\n",
      "363/363 [==============================] - 0s 441us/step - loss: 0.3575 - val_loss: 0.3755\n",
      "Epoch 43/100\n",
      "363/363 [==============================] - 0s 444us/step - loss: 0.3613 - val_loss: 0.3503\n",
      "Epoch 44/100\n",
      "363/363 [==============================] - 0s 444us/step - loss: 0.3469 - val_loss: 0.3378\n",
      "Epoch 45/100\n",
      "363/363 [==============================] - 0s 452us/step - loss: 0.3715 - val_loss: 0.3367\n",
      "Epoch 46/100\n",
      "363/363 [==============================] - 0s 455us/step - loss: 0.3538 - val_loss: 0.3355\n",
      "Epoch 47/100\n",
      "363/363 [==============================] - 0s 470us/step - loss: 0.3570 - val_loss: 0.3515\n",
      "Epoch 48/100\n",
      "363/363 [==============================] - 0s 471us/step - loss: 0.3633 - val_loss: 0.3439\n",
      "Epoch 49/100\n",
      "363/363 [==============================] - 0s 460us/step - loss: 0.3588 - val_loss: 0.3913\n",
      "Epoch 50/100\n",
      "363/363 [==============================] - 0s 454us/step - loss: 0.3551 - val_loss: 0.3521\n",
      "Epoch 51/100\n",
      "363/363 [==============================] - 0s 451us/step - loss: 0.3619 - val_loss: 0.3364\n",
      "Epoch 52/100\n",
      "363/363 [==============================] - 0s 453us/step - loss: 0.3541 - val_loss: 0.3756\n",
      "Epoch 53/100\n",
      "363/363 [==============================] - 0s 451us/step - loss: 0.3630 - val_loss: 0.3630\n",
      "Epoch 54/100\n",
      "363/363 [==============================] - 0s 448us/step - loss: 0.3408 - val_loss: 0.3481\n",
      "Epoch 55/100\n",
      "363/363 [==============================] - 0s 449us/step - loss: 0.3485 - val_loss: 0.3355\n",
      "Epoch 56/100\n",
      "363/363 [==============================] - 0s 448us/step - loss: 0.3472 - val_loss: 0.4019\n",
      "162/162 [==============================] - 0s 283us/step - loss: 0.3524\n",
      "WARNING:tensorflow:5 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f557cf83dc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "keras_reg.fit(X_train, y_train, epochs = 100, validation_data = (X_valid, y_valid), callbacks = [keras.callbacks.EarlyStopping(patience = 10)])\n",
    "mse_test = keras_reg.score(X_test, y_test)\n",
    "y_pred = keras_reg.predict(X_new)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 4.1243 - val_loss: 7.8421\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 1.0700 - val_loss: 2.1575\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.7175 - val_loss: 1.0695\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.6406 - val_loss: 0.7350\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.5846 - val_loss: 0.5996\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.5540 - val_loss: 0.5130\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.5327 - val_loss: 0.4828\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 537us/step - loss: 0.5492 - val_loss: 0.4670\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.4995 - val_loss: 0.4541\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.4582 - val_loss: 0.4571\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 521us/step - loss: 0.4881 - val_loss: 0.4317\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4671 - val_loss: 0.4253\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.4565 - val_loss: 0.4555\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4305 - val_loss: 0.4223\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.4426 - val_loss: 0.4061\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.4366 - val_loss: 0.4025\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.4187 - val_loss: 0.3983\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.4303 - val_loss: 0.4628\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.4175 - val_loss: 0.4080\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.4355 - val_loss: 0.4121\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.4295 - val_loss: 0.3862\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.4179 - val_loss: 0.4462\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.4166 - val_loss: 0.3928\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3952 - val_loss: 0.3931\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.4142 - val_loss: 0.4495\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4046 - val_loss: 0.3881\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.4203 - val_loss: 0.5846\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 563us/step - loss: 0.3822 - val_loss: 0.3784\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3854 - val_loss: 0.4379\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.4087 - val_loss: 0.4140\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3897 - val_loss: 0.3702\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 521us/step - loss: 0.3979 - val_loss: 0.5227\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3989 - val_loss: 0.4760\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3813 - val_loss: 0.5174\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 559us/step - loss: 0.3946 - val_loss: 0.4150\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3917 - val_loss: 0.3733\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3990 - val_loss: 0.4805\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3895 - val_loss: 0.3778\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.3885 - val_loss: 0.6219\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3948 - val_loss: 0.3637\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3729 - val_loss: 0.4427\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3780 - val_loss: 0.4064\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3835 - val_loss: 0.3859\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3826 - val_loss: 0.4291\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.3765 - val_loss: 0.4396\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.3906 - val_loss: 0.3694\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3638 - val_loss: 0.4128\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3966 - val_loss: 0.3947\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.3765 - val_loss: 0.5647\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 532us/step - loss: 0.3816 - val_loss: 0.6049\n",
      "121/121 [==============================] - 0s 283us/step - loss: 0.3963\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 770us/step - loss: 3.4378 - val_loss: 5.3432\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.8318 - val_loss: 2.8732\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.6724 - val_loss: 1.2096\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 526us/step - loss: 0.6283 - val_loss: 0.6308\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.5991 - val_loss: 0.5731\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.5665 - val_loss: 0.7704\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 521us/step - loss: 0.5236 - val_loss: 0.9171\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 531us/step - loss: 0.5175 - val_loss: 1.0320\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.5174 - val_loss: 1.0981\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.5021 - val_loss: 1.0467\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4830 - val_loss: 1.0109\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.4666 - val_loss: 0.9194\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.4491 - val_loss: 0.8211\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.4527 - val_loss: 0.6852\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.4395 - val_loss: 0.6040\n",
      "121/121 [==============================] - 0s 279us/step - loss: 0.4539\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 757us/step - loss: 2.7177 - val_loss: 6.2369\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.7672 - val_loss: 0.7098\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.6499 - val_loss: 0.6140\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.6105 - val_loss: 0.5775\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.5620 - val_loss: 0.5500\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.5704 - val_loss: 0.5275\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.5237 - val_loss: 0.5058\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 537us/step - loss: 0.5107 - val_loss: 0.4842\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4927 - val_loss: 0.4884\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.4844 - val_loss: 0.4636\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.4563 - val_loss: 0.4480\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.4418 - val_loss: 0.4466\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 535us/step - loss: 0.4584 - val_loss: 0.4552\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.4339 - val_loss: 0.4502\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.4499 - val_loss: 0.4321\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 526us/step - loss: 0.4203 - val_loss: 0.4605\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.4241 - val_loss: 0.4688\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4062 - val_loss: 0.4231\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.4133 - val_loss: 0.4282\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4213 - val_loss: 0.4353\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.4372 - val_loss: 0.4094\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.4073 - val_loss: 0.4268\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 530us/step - loss: 0.4032 - val_loss: 0.4474\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.4219 - val_loss: 0.4301\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 599us/step - loss: 0.4022 - val_loss: 0.4091\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 521us/step - loss: 0.3962 - val_loss: 0.4458\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3937 - val_loss: 0.4554\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.4037 - val_loss: 0.3934\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.4092 - val_loss: 0.4079\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.3813 - val_loss: 0.3889\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3998 - val_loss: 0.4339\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 541us/step - loss: 0.3822 - val_loss: 0.4427\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 538us/step - loss: 0.4028 - val_loss: 0.4299\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.3940 - val_loss: 0.3907\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3885 - val_loss: 0.3982\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.3866 - val_loss: 0.4042\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3827 - val_loss: 0.4332\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.4154 - val_loss: 0.3978\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3801 - val_loss: 0.3880\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.3815 - val_loss: 0.3915\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.4001 - val_loss: 0.4174\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3905 - val_loss: 0.4181\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.3851 - val_loss: 0.4253\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3887 - val_loss: 0.4169\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3813 - val_loss: 0.3910\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3726 - val_loss: 0.3880\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.3918 - val_loss: 0.4176\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 565us/step - loss: 0.3741 - val_loss: 0.4100\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.3846 - val_loss: 0.4157\n",
      "121/121 [==============================] - 0s 301us/step - loss: 0.3780\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 801us/step - loss: 2.1432 - val_loss: 55.4735\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 541us/step - loss: 4.0693 - val_loss: 18.6055\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 534us/step - loss: 0.5890 - val_loss: 6.4691\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 531us/step - loss: 0.5390 - val_loss: 0.4472\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 537us/step - loss: 0.4925 - val_loss: 0.4220\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 543us/step - loss: 0.4553 - val_loss: 0.4134\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.4333 - val_loss: 0.4163\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 532us/step - loss: 0.4143 - val_loss: 0.4046\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 0.4006 - val_loss: 0.3942\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 530us/step - loss: 0.3818 - val_loss: 0.4106\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.4088 - val_loss: 0.3985\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.4031 - val_loss: 0.4028\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3904 - val_loss: 0.3869\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.3886 - val_loss: 0.3895\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 539us/step - loss: 0.3910 - val_loss: 0.4045\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4177 - val_loss: 0.3781\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 531us/step - loss: 0.3845 - val_loss: 0.3751\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.3672 - val_loss: 0.3954\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 550us/step - loss: 0.3738 - val_loss: 0.3792\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3623 - val_loss: 0.3852\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.3904 - val_loss: 0.3954\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3697 - val_loss: 0.3721\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.3941 - val_loss: 0.3878\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3684 - val_loss: 0.3684\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.3617 - val_loss: 0.3683\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3795 - val_loss: 0.3591\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.3621 - val_loss: 0.3624\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3578 - val_loss: 0.3822\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3875 - val_loss: 0.3504\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3548 - val_loss: 0.3552\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 533us/step - loss: 0.3420 - val_loss: 0.4043\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3732 - val_loss: 0.3500\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 531us/step - loss: 0.3510 - val_loss: 0.3849\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3429 - val_loss: 0.3681\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 549us/step - loss: 0.3507 - val_loss: 0.4038\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 530us/step - loss: 0.3396 - val_loss: 0.3660\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.3600 - val_loss: 0.3768\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3673 - val_loss: 0.3906\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.3390 - val_loss: 0.3749\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.3336 - val_loss: 0.3710\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3518 - val_loss: 0.3736\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 560us/step - loss: 0.3468 - val_loss: 0.3340\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3571 - val_loss: 0.3509\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 555us/step - loss: 0.3434 - val_loss: 0.3725\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.3440 - val_loss: 0.3303\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 560us/step - loss: 0.3318 - val_loss: 0.3357\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 537us/step - loss: 0.3308 - val_loss: 0.3328\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 547us/step - loss: 0.3274 - val_loss: 0.3787\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 600us/step - loss: 0.3332 - val_loss: 0.3311\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 619us/step - loss: 0.3499 - val_loss: 0.3880\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.3341 - val_loss: 0.3265\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 728us/step - loss: 0.3462 - val_loss: 0.3268\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 663us/step - loss: 0.3273 - val_loss: 0.3269\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 600us/step - loss: 0.3271 - val_loss: 0.3326\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 548us/step - loss: 0.3421 - val_loss: 0.4587\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 553us/step - loss: 0.3366 - val_loss: 0.4906\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 534us/step - loss: 0.3403 - val_loss: 0.4921\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 567us/step - loss: 0.3321 - val_loss: 0.3335\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.3238 - val_loss: 0.3547\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.3362 - val_loss: 0.3351\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 598us/step - loss: 0.3358 - val_loss: 0.3934\n",
      "121/121 [==============================] - 0s 385us/step - loss: 0.3610\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 938us/step - loss: 2.3120 - val_loss: 1.0521\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.5766 - val_loss: 0.5722\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.5124 - val_loss: 0.8469\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 544us/step - loss: 0.4588 - val_loss: 0.9000\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 537us/step - loss: 0.4523 - val_loss: 0.6559\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.4311 - val_loss: 0.4889\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.4096 - val_loss: 0.3922\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.4097 - val_loss: 0.3775\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.4149 - val_loss: 0.4129\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 591us/step - loss: 0.3991 - val_loss: 0.5568\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 598us/step - loss: 0.3945 - val_loss: 0.7799\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.4028 - val_loss: 0.7672\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 0.3710 - val_loss: 0.9348\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 700us/step - loss: 0.3784 - val_loss: 1.0350\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 0.3881 - val_loss: 1.0273\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 598us/step - loss: 0.3722 - val_loss: 1.0617\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.3723 - val_loss: 1.1277\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3587 - val_loss: 1.2904\n",
      "121/121 [==============================] - 0s 260us/step - loss: 0.3976\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 773us/step - loss: 2.0290 - val_loss: 12.9662\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.7311 - val_loss: 11.0351\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.5618 - val_loss: 1.2415\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.4971 - val_loss: 0.4315\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.4669 - val_loss: 0.4287\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4492 - val_loss: 0.4228\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.4200 - val_loss: 0.4341\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 552us/step - loss: 0.4304 - val_loss: 0.3987\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 644us/step - loss: 0.3807 - val_loss: 0.4034\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.4109 - val_loss: 0.4123\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 587us/step - loss: 0.3959 - val_loss: 0.4638\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 543us/step - loss: 0.3841 - val_loss: 0.4327\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3990 - val_loss: 0.4406\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3922 - val_loss: 0.3783\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3965 - val_loss: 0.4567\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.3745 - val_loss: 0.3671\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3881 - val_loss: 0.3821\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 843us/step - loss: 0.3769 - val_loss: 0.3618\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 557us/step - loss: 0.3910 - val_loss: 0.4059\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.3764 - val_loss: 0.3517\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 558us/step - loss: 0.3707 - val_loss: 0.4404\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 649us/step - loss: 0.3749 - val_loss: 0.3480\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 549us/step - loss: 0.3682 - val_loss: 0.4908\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 582us/step - loss: 0.3686 - val_loss: 0.3472\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 530us/step - loss: 0.3660 - val_loss: 0.3639\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.3723 - val_loss: 0.3576\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 567us/step - loss: 0.3582 - val_loss: 0.4379\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.3609 - val_loss: 0.3844\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.3665 - val_loss: 0.3875\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3558 - val_loss: 0.3448\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.3739 - val_loss: 0.3411\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3387 - val_loss: 0.3495\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.3573 - val_loss: 0.6242\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.3695 - val_loss: 0.4570\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3761 - val_loss: 0.5698\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3716 - val_loss: 0.3536\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3570 - val_loss: 0.3363\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3399 - val_loss: 0.5403\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.3576 - val_loss: 0.3524\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.3472 - val_loss: 0.3368\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3474 - val_loss: 0.4845\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.3627 - val_loss: 0.3616\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3530 - val_loss: 0.3333\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3366 - val_loss: 0.3768\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3460 - val_loss: 0.4391\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.3476 - val_loss: 0.3461\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 531us/step - loss: 0.3325 - val_loss: 0.4534\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 526us/step - loss: 0.3552 - val_loss: 0.3416\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3427 - val_loss: 0.5410\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 540us/step - loss: 0.3669 - val_loss: 0.3303\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3602 - val_loss: 0.4325\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 538us/step - loss: 0.3381 - val_loss: 0.3513\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3316 - val_loss: 0.4993\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.3404 - val_loss: 0.3713\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.3530 - val_loss: 0.7023\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 548us/step - loss: 0.3340 - val_loss: 0.3247\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3458 - val_loss: 0.5464\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3439 - val_loss: 0.3957\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.3316 - val_loss: 1.2210\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3597 - val_loss: 0.8277\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3640 - val_loss: 1.7952\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3879 - val_loss: 1.0233\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.3373 - val_loss: 2.4823\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3456 - val_loss: 1.8218\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.3428 - val_loss: 1.9506\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.3445 - val_loss: 0.5654\n",
      "121/121 [==============================] - 0s 282us/step - loss: 0.3427\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 741us/step - loss: 3.8655 - val_loss: 2.5530\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 1.5521 - val_loss: 1.2988\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 483us/step - loss: 1.0021 - val_loss: 0.8578\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.8364 - val_loss: 0.7557\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.7367 - val_loss: 0.7183\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.6962 - val_loss: 0.6800\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 562us/step - loss: 0.7271 - val_loss: 0.6725\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.6832 - val_loss: 0.6490\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.6421 - val_loss: 0.6199\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.6247 - val_loss: 0.6121\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.5874 - val_loss: 0.5991\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 619us/step - loss: 0.5984 - val_loss: 0.5812\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.6004 - val_loss: 0.5741\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.5802 - val_loss: 0.5553\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.6011 - val_loss: 0.5491\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.5708 - val_loss: 0.5394\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.5578 - val_loss: 0.5272\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 532us/step - loss: 0.5355 - val_loss: 0.5180\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 561us/step - loss: 0.5259 - val_loss: 0.5103\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.5220 - val_loss: 0.5026\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 532us/step - loss: 0.5237 - val_loss: 0.4963\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 526us/step - loss: 0.4925 - val_loss: 0.4898\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.5179 - val_loss: 0.4835\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.5179 - val_loss: 0.4777\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.4829 - val_loss: 0.4723\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.4863 - val_loss: 0.4676\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.5024 - val_loss: 0.4625\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.4753 - val_loss: 0.4581\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.4628 - val_loss: 0.4561\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4732 - val_loss: 0.4497\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.4819 - val_loss: 0.4466\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.4572 - val_loss: 0.4449\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4566 - val_loss: 0.4419\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.4522 - val_loss: 0.4390\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.4434 - val_loss: 0.4370\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.4605 - val_loss: 0.4352\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.4398 - val_loss: 0.4313\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.4515 - val_loss: 0.4322\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.4517 - val_loss: 0.4288\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.4527 - val_loss: 0.4280\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.4461 - val_loss: 0.4258\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.4404 - val_loss: 0.4193\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4330 - val_loss: 0.4222\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4346 - val_loss: 0.4187\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4336 - val_loss: 0.4194\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 545us/step - loss: 0.4401 - val_loss: 0.4170\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.4094 - val_loss: 0.4186\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.4375 - val_loss: 0.4166\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4513 - val_loss: 0.4140\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4312 - val_loss: 0.4131\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.4174 - val_loss: 0.4158\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.4344 - val_loss: 0.4141\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.4214 - val_loss: 0.4120\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.4401 - val_loss: 0.4140\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4204 - val_loss: 0.4129\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.4167 - val_loss: 0.4108\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4149 - val_loss: 0.4099\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4295 - val_loss: 0.4107\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.4257 - val_loss: 0.4100\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4076 - val_loss: 0.4121\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.4124 - val_loss: 0.4098\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.4175 - val_loss: 0.4073\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3920 - val_loss: 0.4122\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3951 - val_loss: 0.4135\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.4295 - val_loss: 0.4062\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4006 - val_loss: 0.4141\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3918 - val_loss: 0.4095\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3897 - val_loss: 0.4068\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3971 - val_loss: 0.4021\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4094 - val_loss: 0.4032\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.4068 - val_loss: 0.4087\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 481us/step - loss: 0.4014 - val_loss: 0.4097\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3956 - val_loss: 0.4048\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.3908 - val_loss: 0.4047\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.4089 - val_loss: 0.3996\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3995 - val_loss: 0.4009\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.4063 - val_loss: 0.4078\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3897 - val_loss: 0.4043\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.3941 - val_loss: 0.4061\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.4067 - val_loss: 0.4016\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4015 - val_loss: 0.4000\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.3891 - val_loss: 0.4055\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.3897 - val_loss: 0.4015\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.3917 - val_loss: 0.4033\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3883 - val_loss: 0.4015\n",
      "121/121 [==============================] - 0s 267us/step - loss: 0.4042\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 740us/step - loss: 4.2401 - val_loss: 13.9588\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 1.4674 - val_loss: 13.7986\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 1.0587 - val_loss: 10.7951\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.8752 - val_loss: 8.0882\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.8262 - val_loss: 5.8568\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.8098 - val_loss: 4.1904\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.7805 - val_loss: 3.0102\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.7177 - val_loss: 2.1855\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.7438 - val_loss: 1.5700\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 0.6943 - val_loss: 1.1583\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.6918 - val_loss: 0.8815\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.6387 - val_loss: 0.7115\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.6392 - val_loss: 0.6143\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.6064 - val_loss: 0.5822\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.6020 - val_loss: 0.5911\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.6120 - val_loss: 0.6279\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.5816 - val_loss: 0.6875\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.5597 - val_loss: 0.7513\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.5703 - val_loss: 0.8207\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.5220 - val_loss: 0.8853\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.5181 - val_loss: 0.9435\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.5092 - val_loss: 1.0003\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.4985 - val_loss: 1.0518\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.4915 - val_loss: 1.0826\n",
      "121/121 [==============================] - 0s 269us/step - loss: 0.5274\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 732us/step - loss: 4.2516 - val_loss: 2.0290\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 1.3060 - val_loss: 1.1573\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.8464 - val_loss: 0.8120\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.7246 - val_loss: 0.6994\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.6884 - val_loss: 0.6545\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.6342 - val_loss: 0.6343\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.6197 - val_loss: 0.6085\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.6181 - val_loss: 0.5822\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 605us/step - loss: 0.5777 - val_loss: 0.5627\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 533us/step - loss: 0.5444 - val_loss: 0.5472\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.5413 - val_loss: 0.5324\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.5273 - val_loss: 0.5188\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.5417 - val_loss: 0.5082\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.5218 - val_loss: 0.4987\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.5178 - val_loss: 0.4902\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.5093 - val_loss: 0.4799\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 582us/step - loss: 0.5124 - val_loss: 0.4745\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.5155 - val_loss: 0.4681\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.4731 - val_loss: 0.4637\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4900 - val_loss: 0.4584\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.4672 - val_loss: 0.4552\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.4486 - val_loss: 0.4529\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.4610 - val_loss: 0.4498\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4542 - val_loss: 0.4466\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.4304 - val_loss: 0.4446\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4567 - val_loss: 0.4389\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.4766 - val_loss: 0.4391\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4515 - val_loss: 0.4397\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.4479 - val_loss: 0.4396\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4235 - val_loss: 0.4379\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 531us/step - loss: 0.4396 - val_loss: 0.4390\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4650 - val_loss: 0.4403\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 544us/step - loss: 0.4546 - val_loss: 0.4345\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.4232 - val_loss: 0.4356\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.4529 - val_loss: 0.4337\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.4171 - val_loss: 0.4326\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.4364 - val_loss: 0.4311\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.4114 - val_loss: 0.4291\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 539us/step - loss: 0.4221 - val_loss: 0.4277\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 547us/step - loss: 0.4251 - val_loss: 0.4335\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 553us/step - loss: 0.4339 - val_loss: 0.4315\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.4398 - val_loss: 0.4352\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 530us/step - loss: 0.4289 - val_loss: 0.4257\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.4364 - val_loss: 0.4267\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 594us/step - loss: 0.4325 - val_loss: 0.4341\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.4149 - val_loss: 0.4376\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.4084 - val_loss: 0.4341\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4248 - val_loss: 0.4285\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4087 - val_loss: 0.4229\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4247 - val_loss: 0.4271\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4203 - val_loss: 0.4347\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4259 - val_loss: 0.4278\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.4318 - val_loss: 0.4297\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 541us/step - loss: 0.4143 - val_loss: 0.4337\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.4113 - val_loss: 0.4275\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4145 - val_loss: 0.4228\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 0.4069 - val_loss: 0.4175\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.4206 - val_loss: 0.4200\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 648us/step - loss: 0.4073 - val_loss: 0.4224\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.4024 - val_loss: 0.4247\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3895 - val_loss: 0.4228\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 546us/step - loss: 0.4190 - val_loss: 0.4260\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 556us/step - loss: 0.4045 - val_loss: 0.4217\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.4215 - val_loss: 0.4137\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.4070 - val_loss: 0.4103\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.4140 - val_loss: 0.4193\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 591us/step - loss: 0.3955 - val_loss: 0.4098\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 548us/step - loss: 0.3974 - val_loss: 0.4097\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 636us/step - loss: 0.4013 - val_loss: 0.4163\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 0.4060 - val_loss: 0.4081\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 539us/step - loss: 0.4210 - val_loss: 0.4160\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 538us/step - loss: 0.3952 - val_loss: 0.4258\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 521us/step - loss: 0.3944 - val_loss: 0.4228\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.4098 - val_loss: 0.4133\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 550us/step - loss: 0.3981 - val_loss: 0.4064\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3980 - val_loss: 0.4010\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.3994 - val_loss: 0.4082\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3886 - val_loss: 0.4058\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.4060 - val_loss: 0.4031\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 556us/step - loss: 0.4024 - val_loss: 0.4029\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.4137 - val_loss: 0.4106\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 563us/step - loss: 0.4139 - val_loss: 0.4125\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 546us/step - loss: 0.3749 - val_loss: 0.4055\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 561us/step - loss: 0.4009 - val_loss: 0.4072\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3992 - val_loss: 0.4093\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 545us/step - loss: 0.3909 - val_loss: 0.4091\n",
      "121/121 [==============================] - 0s 288us/step - loss: 0.3879\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 868us/step - loss: 3.6050 - val_loss: 5.9362\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 598us/step - loss: 1.5775 - val_loss: 3.8142\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.9936 - val_loss: 1.5224\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 1ms/step - loss: 0.8303 - val_loss: 0.9599\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 683us/step - loss: 0.7744 - val_loss: 0.7630\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 622us/step - loss: 0.7245 - val_loss: 0.6945\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.7008 - val_loss: 0.6501\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.6644 - val_loss: 0.6251\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.6282 - val_loss: 0.6078\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 592us/step - loss: 0.6164 - val_loss: 0.5934\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.6481 - val_loss: 0.5808\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.6183 - val_loss: 0.5682\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.5797 - val_loss: 0.5571\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 600us/step - loss: 0.5690 - val_loss: 0.5471\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.5837 - val_loss: 0.5377\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.5962 - val_loss: 0.5294\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 703us/step - loss: 0.5532 - val_loss: 0.5226\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 696us/step - loss: 0.5687 - val_loss: 0.5137\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 776us/step - loss: 0.5177 - val_loss: 0.5076\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 593us/step - loss: 0.5336 - val_loss: 0.5000\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.5161 - val_loss: 0.4948\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.5047 - val_loss: 0.4908\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 677us/step - loss: 0.5309 - val_loss: 0.4847\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.5271 - val_loss: 0.4803\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 597us/step - loss: 0.5135 - val_loss: 0.4766\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.4766 - val_loss: 0.4740\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.4795 - val_loss: 0.4668\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 594us/step - loss: 0.4840 - val_loss: 0.4637\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 600us/step - loss: 0.4629 - val_loss: 0.4619\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 591us/step - loss: 0.4734 - val_loss: 0.4593\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.4855 - val_loss: 0.4571\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 592us/step - loss: 0.4695 - val_loss: 0.4573\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 587us/step - loss: 0.4362 - val_loss: 0.4555\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 594us/step - loss: 0.4696 - val_loss: 0.4514\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 639us/step - loss: 0.4739 - val_loss: 0.4496\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 598us/step - loss: 0.4587 - val_loss: 0.4408\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.4271 - val_loss: 0.4397\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 601us/step - loss: 0.4396 - val_loss: 0.4409\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.4566 - val_loss: 0.4379\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 593us/step - loss: 0.4219 - val_loss: 0.4429\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 596us/step - loss: 0.4427 - val_loss: 0.4314\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 605us/step - loss: 0.4160 - val_loss: 0.4396\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.4148 - val_loss: 0.4329\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3986 - val_loss: 0.4320\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.4426 - val_loss: 0.4357\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.4321 - val_loss: 0.4276\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.4195 - val_loss: 0.4271\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 593us/step - loss: 0.4148 - val_loss: 0.4267\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 613us/step - loss: 0.4061 - val_loss: 0.4268\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.4118 - val_loss: 0.4268\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 592us/step - loss: 0.4000 - val_loss: 0.4231\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.3961 - val_loss: 0.4294\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 591us/step - loss: 0.3937 - val_loss: 0.4244\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.4069 - val_loss: 0.4230\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4143 - val_loss: 0.4220\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.3739 - val_loss: 0.4249\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.4044 - val_loss: 0.4266\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.4083 - val_loss: 0.4274\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.4181 - val_loss: 0.4359\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3995 - val_loss: 0.4279\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.4093 - val_loss: 0.4218\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 633us/step - loss: 0.4067 - val_loss: 0.4277\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 597us/step - loss: 0.3604 - val_loss: 0.4219\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.4072 - val_loss: 0.4172\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3875 - val_loss: 0.4159\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3891 - val_loss: 0.4118\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3761 - val_loss: 0.4201\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.3777 - val_loss: 0.4152\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3703 - val_loss: 0.4096\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3832 - val_loss: 0.4143\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 590us/step - loss: 0.3824 - val_loss: 0.4193\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.3776 - val_loss: 0.4095\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3790 - val_loss: 0.4033\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.3781 - val_loss: 0.4044\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3770 - val_loss: 0.4015\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3889 - val_loss: 0.4063\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 581us/step - loss: 0.3718 - val_loss: 0.4170\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3717 - val_loss: 0.4014\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.3601 - val_loss: 0.4076\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 609us/step - loss: 0.3816 - val_loss: 0.4060\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 594us/step - loss: 0.3630 - val_loss: 0.4121\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.3567 - val_loss: 0.4032\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3629 - val_loss: 0.4027\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.3718 - val_loss: 0.3977\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 593us/step - loss: 0.3623 - val_loss: 0.4019\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.3728 - val_loss: 0.3991\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 581us/step - loss: 0.3473 - val_loss: 0.3948\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3784 - val_loss: 0.4010\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 593us/step - loss: 0.3756 - val_loss: 0.4097\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.3636 - val_loss: 0.4009\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3620 - val_loss: 0.3964\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.3442 - val_loss: 0.3878\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3743 - val_loss: 0.3989\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.3563 - val_loss: 0.3927\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 582us/step - loss: 0.3616 - val_loss: 0.3866\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 581us/step - loss: 0.3718 - val_loss: 0.3925\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 642us/step - loss: 0.3664 - val_loss: 0.3843\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.3552 - val_loss: 0.3950\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3526 - val_loss: 0.3852\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.3428 - val_loss: 0.3854\n",
      "121/121 [==============================] - 0s 305us/step - loss: 0.3717\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 852us/step - loss: 4.6335 - val_loss: 8.1156\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 590us/step - loss: 1.8321 - val_loss: 13.4161\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 1.1698 - val_loss: 12.7737\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.9391 - val_loss: 10.7777\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.8725 - val_loss: 9.1046\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.7972 - val_loss: 7.3855\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.7796 - val_loss: 6.2682\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 601us/step - loss: 0.7129 - val_loss: 5.2981\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.7105 - val_loss: 4.2346\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.6739 - val_loss: 3.6410\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.6675 - val_loss: 3.0208\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.6261 - val_loss: 2.6505\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 637us/step - loss: 0.6413 - val_loss: 2.3757\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.6364 - val_loss: 2.0737\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 596us/step - loss: 0.6052 - val_loss: 1.8393\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.6063 - val_loss: 1.6037\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.5988 - val_loss: 1.4413\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.5881 - val_loss: 1.3088\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.5831 - val_loss: 1.1648\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.5628 - val_loss: 1.0430\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.5634 - val_loss: 0.9503\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.5461 - val_loss: 0.8716\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.5363 - val_loss: 0.7989\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 581us/step - loss: 0.5749 - val_loss: 0.7298\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 590us/step - loss: 0.5492 - val_loss: 0.6848\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.5183 - val_loss: 0.6521\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.5174 - val_loss: 0.6130\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.4965 - val_loss: 0.5859\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 635us/step - loss: 0.5163 - val_loss: 0.5648\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.5037 - val_loss: 0.5427\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.4835 - val_loss: 0.5155\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.4864 - val_loss: 0.4973\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.4770 - val_loss: 0.4951\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 596us/step - loss: 0.4856 - val_loss: 0.4810\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.4829 - val_loss: 0.4673\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 603us/step - loss: 0.4802 - val_loss: 0.4553\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4642 - val_loss: 0.4483\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.4803 - val_loss: 0.4416\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.4575 - val_loss: 0.4360\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.4474 - val_loss: 0.4320\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.4669 - val_loss: 0.4260\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.4628 - val_loss: 0.4226\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.4326 - val_loss: 0.4202\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.4460 - val_loss: 0.4171\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.4188 - val_loss: 0.4143\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 587us/step - loss: 0.4123 - val_loss: 0.4120\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.4224 - val_loss: 0.4096\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.4349 - val_loss: 0.4077\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4115 - val_loss: 0.4054\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.4296 - val_loss: 0.4037\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4116 - val_loss: 0.4019\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.4253 - val_loss: 0.4000\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.4300 - val_loss: 0.3982\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.4288 - val_loss: 0.3965\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.4221 - val_loss: 0.3954\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 599us/step - loss: 0.4227 - val_loss: 0.3931\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.4298 - val_loss: 0.3909\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.4058 - val_loss: 0.3886\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.4131 - val_loss: 0.3868\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 632us/step - loss: 0.4192 - val_loss: 0.3848\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.3931 - val_loss: 0.3836\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.3957 - val_loss: 0.3815\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3943 - val_loss: 0.3796\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.3877 - val_loss: 0.3780\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4003 - val_loss: 0.3764\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3873 - val_loss: 0.3749\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3983 - val_loss: 0.3735\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 566us/step - loss: 0.3812 - val_loss: 0.3725\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3883 - val_loss: 0.3710\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.3797 - val_loss: 0.3698\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.3889 - val_loss: 0.3700\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3777 - val_loss: 0.3687\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.4061 - val_loss: 0.3677\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3708 - val_loss: 0.3692\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 627us/step - loss: 0.3881 - val_loss: 0.3683\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.3797 - val_loss: 0.3668\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.3833 - val_loss: 0.3686\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 565us/step - loss: 0.3766 - val_loss: 0.3715\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.3696 - val_loss: 0.3723\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.3887 - val_loss: 0.3687\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.3860 - val_loss: 0.3739\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 587us/step - loss: 0.3673 - val_loss: 0.3722\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 587us/step - loss: 0.3636 - val_loss: 0.3736\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 602us/step - loss: 0.3792 - val_loss: 0.3792\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.3757 - val_loss: 0.3804\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.3780 - val_loss: 0.3834\n",
      "121/121 [==============================] - 0s 294us/step - loss: 0.3855\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 833us/step - loss: 4.9275 - val_loss: 3.5122\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 1.8426 - val_loss: 4.1674\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.9647 - val_loss: 2.9306\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 638us/step - loss: 0.7915 - val_loss: 1.9054\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.7327 - val_loss: 1.3444\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.6781 - val_loss: 1.1110\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 565us/step - loss: 0.6667 - val_loss: 0.9242\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 568us/step - loss: 0.6443 - val_loss: 0.8100\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.6245 - val_loss: 0.7216\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.6034 - val_loss: 0.6656\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.6143 - val_loss: 0.6235\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.5777 - val_loss: 0.5882\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 673us/step - loss: 0.6009 - val_loss: 0.5648\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.5809 - val_loss: 0.5439\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.5259 - val_loss: 0.5261\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 568us/step - loss: 0.5313 - val_loss: 0.5133\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 565us/step - loss: 0.5363 - val_loss: 0.5008\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.5042 - val_loss: 0.4907\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.5059 - val_loss: 0.4809\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.4873 - val_loss: 0.4743\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.4908 - val_loss: 0.4683\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.4721 - val_loss: 0.4605\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.4757 - val_loss: 0.4574\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.4726 - val_loss: 0.4480\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.4927 - val_loss: 0.4453\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.4723 - val_loss: 0.4404\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.4607 - val_loss: 0.4353\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.4696 - val_loss: 0.4342\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 566us/step - loss: 0.4653 - val_loss: 0.4296\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 558us/step - loss: 0.4553 - val_loss: 0.4263\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.4437 - val_loss: 0.4230\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.4361 - val_loss: 0.4226\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.4153 - val_loss: 0.4213\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.4379 - val_loss: 0.4191\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 618us/step - loss: 0.4366 - val_loss: 0.4146\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4263 - val_loss: 0.4136\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 565us/step - loss: 0.3956 - val_loss: 0.4143\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.4224 - val_loss: 0.4127\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.4227 - val_loss: 0.4092\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4275 - val_loss: 0.4082\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.4209 - val_loss: 0.4099\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.3971 - val_loss: 0.4114\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 578us/step - loss: 0.4022 - val_loss: 0.4120\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.4277 - val_loss: 0.4130\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.4014 - val_loss: 0.4081\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 593us/step - loss: 0.3923 - val_loss: 0.4095\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3933 - val_loss: 0.4074\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.4242 - val_loss: 0.4056\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.4157 - val_loss: 0.4071\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.4166 - val_loss: 0.4081\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.4022 - val_loss: 0.4050\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 607us/step - loss: 0.3948 - val_loss: 0.4058\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 568us/step - loss: 0.3808 - val_loss: 0.4078\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 581us/step - loss: 0.3802 - val_loss: 0.4010\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.4009 - val_loss: 0.4040\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.3805 - val_loss: 0.4034\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 572us/step - loss: 0.4003 - val_loss: 0.3997\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3843 - val_loss: 0.4018\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.3751 - val_loss: 0.3986\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 565us/step - loss: 0.3955 - val_loss: 0.3982\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.4017 - val_loss: 0.3983\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.3725 - val_loss: 0.3976\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 570us/step - loss: 0.3613 - val_loss: 0.4015\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.3683 - val_loss: 0.4002\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 566us/step - loss: 0.3783 - val_loss: 0.4031\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.3905 - val_loss: 0.4075\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 668us/step - loss: 0.3720 - val_loss: 0.4046\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 590us/step - loss: 0.3701 - val_loss: 0.3980\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 586us/step - loss: 0.3682 - val_loss: 0.3970\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.3707 - val_loss: 0.4017\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.3829 - val_loss: 0.4056\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3875 - val_loss: 0.4012\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.3619 - val_loss: 0.4002\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.3692 - val_loss: 0.3992\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3682 - val_loss: 0.4024\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.3766 - val_loss: 0.4011\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 585us/step - loss: 0.3710 - val_loss: 0.4009\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.3785 - val_loss: 0.3990\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3965 - val_loss: 0.3949\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.3711 - val_loss: 0.3970\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 610us/step - loss: 0.3723 - val_loss: 0.3948\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3708 - val_loss: 0.3955\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3582 - val_loss: 0.3962\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 568us/step - loss: 0.3795 - val_loss: 0.3914\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 582us/step - loss: 0.3738 - val_loss: 0.3939\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 553us/step - loss: 0.3642 - val_loss: 0.3930\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 576us/step - loss: 0.3777 - val_loss: 0.3938\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 567us/step - loss: 0.3549 - val_loss: 0.3943\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.3520 - val_loss: 0.3944\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.3823 - val_loss: 0.3938\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 582us/step - loss: 0.3509 - val_loss: 0.3950\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 574us/step - loss: 0.3534 - val_loss: 0.3972\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 588us/step - loss: 0.3544 - val_loss: 0.3993\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.3685 - val_loss: 0.3900\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 564us/step - loss: 0.3520 - val_loss: 0.3878\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 623us/step - loss: 0.3424 - val_loss: 0.3824\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 573us/step - loss: 0.3567 - val_loss: 0.3933\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.3454 - val_loss: 0.3951\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 581us/step - loss: 0.3767 - val_loss: 0.3906\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 582us/step - loss: 0.3682 - val_loss: 0.3848\n",
      "121/121 [==============================] - 0s 281us/step - loss: 0.3558\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 683us/step - loss: 9.3756 - val_loss: 11.9480\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 5.3366 - val_loss: 6.1961\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 3.3672 - val_loss: 3.4012\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 476us/step - loss: 2.2728 - val_loss: 2.0080\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 1.6093 - val_loss: 1.3357\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 1.1924 - val_loss: 1.0078\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 1.0108 - val_loss: 0.8647\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.8206 - val_loss: 0.7986\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.7891 - val_loss: 0.7909\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.7235 - val_loss: 0.8112\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.6711 - val_loss: 0.7986\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.6543 - val_loss: 0.8013\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 465us/step - loss: 0.6524 - val_loss: 0.8386\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.6353 - val_loss: 0.8428\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 483us/step - loss: 0.6361 - val_loss: 0.8194\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 486us/step - loss: 0.6406 - val_loss: 0.8450\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 481us/step - loss: 0.6088 - val_loss: 0.8436\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 483us/step - loss: 0.5904 - val_loss: 0.8713\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.5841 - val_loss: 0.8551\n",
      "121/121 [==============================] - 0s 254us/step - loss: 0.5958\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 699us/step - loss: 6.8969 - val_loss: 58.6580\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 3.8273 - val_loss: 58.0922\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 2.3654 - val_loss: 57.3910\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 1.6240 - val_loss: 56.4625\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 478us/step - loss: 1.2314 - val_loss: 55.3717\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 477us/step - loss: 0.9891 - val_loss: 54.1580\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.8814 - val_loss: 52.9289\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.7569 - val_loss: 51.6814\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.6894 - val_loss: 50.4138\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.7364 - val_loss: 49.1830\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.6306 - val_loss: 47.9848\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 474us/step - loss: 0.5962 - val_loss: 46.8077\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 461us/step - loss: 0.6145 - val_loss: 45.6868\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.6313 - val_loss: 44.6256\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.5940 - val_loss: 43.5784\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.5708 - val_loss: 42.5759\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.6019 - val_loss: 41.6553\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.6130 - val_loss: 40.7690\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.5964 - val_loss: 39.9305\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 477us/step - loss: 0.5734 - val_loss: 39.1254\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 469us/step - loss: 0.5941 - val_loss: 38.3411\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.5567 - val_loss: 37.6175\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.5563 - val_loss: 36.9181\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 480us/step - loss: 0.5881 - val_loss: 36.2665\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.5763 - val_loss: 35.6403\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.5612 - val_loss: 35.0424\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.5660 - val_loss: 34.4607\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 480us/step - loss: 0.5511 - val_loss: 33.9096\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 459us/step - loss: 0.5525 - val_loss: 33.3940\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.5303 - val_loss: 32.8869\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.5317 - val_loss: 32.4005\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.5666 - val_loss: 31.9346\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.5434 - val_loss: 31.5065\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 474us/step - loss: 0.5540 - val_loss: 31.1060\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.5481 - val_loss: 30.6987\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.5689 - val_loss: 30.3343\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 474us/step - loss: 0.5073 - val_loss: 29.9563\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.5349 - val_loss: 29.6239\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 476us/step - loss: 0.5243 - val_loss: 29.2806\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 480us/step - loss: 0.5743 - val_loss: 28.9783\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.5309 - val_loss: 28.6920\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 477us/step - loss: 0.5580 - val_loss: 28.4143\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.5273 - val_loss: 28.1213\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 469us/step - loss: 0.5104 - val_loss: 27.8556\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.5231 - val_loss: 27.6088\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.5125 - val_loss: 27.3663\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.5350 - val_loss: 27.1266\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.5289 - val_loss: 26.8940\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 466us/step - loss: 0.5150 - val_loss: 26.6715\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.5027 - val_loss: 26.4496\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 472us/step - loss: 0.5281 - val_loss: 26.2576\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.5238 - val_loss: 26.0645\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 461us/step - loss: 0.5108 - val_loss: 25.8728\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.5057 - val_loss: 25.6853\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 457us/step - loss: 0.4938 - val_loss: 25.5132\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 469us/step - loss: 0.5020 - val_loss: 25.3497\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.5104 - val_loss: 25.1953\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 472us/step - loss: 0.5192 - val_loss: 25.0366\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 459us/step - loss: 0.5162 - val_loss: 24.8947\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.4924 - val_loss: 24.7409\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 474us/step - loss: 0.4835 - val_loss: 24.6044\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.5126 - val_loss: 24.4662\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 481us/step - loss: 0.4971 - val_loss: 24.3449\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.4975 - val_loss: 24.2317\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.5011 - val_loss: 24.1155\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.5097 - val_loss: 23.9967\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 461us/step - loss: 0.5344 - val_loss: 23.8951\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.5206 - val_loss: 23.7995\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.5268 - val_loss: 23.7044\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 457us/step - loss: 0.4907 - val_loss: 23.6079\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.5096 - val_loss: 23.5118\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.4763 - val_loss: 23.4334\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.5219 - val_loss: 23.3521\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.5169 - val_loss: 23.2793\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.5147 - val_loss: 23.1809\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 457us/step - loss: 0.5156 - val_loss: 23.0897\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 459us/step - loss: 0.5241 - val_loss: 23.0138\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.5368 - val_loss: 22.9414\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.5053 - val_loss: 22.8766\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.5264 - val_loss: 22.8129\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.5173 - val_loss: 22.7524\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 466us/step - loss: 0.5148 - val_loss: 22.6760\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.4841 - val_loss: 22.5977\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.5320 - val_loss: 22.5412\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.5061 - val_loss: 22.5004\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.5122 - val_loss: 22.4445\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 478us/step - loss: 0.4849 - val_loss: 22.3899\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 477us/step - loss: 0.5286 - val_loss: 22.3396\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 479us/step - loss: 0.5017 - val_loss: 22.2901\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 466us/step - loss: 0.4952 - val_loss: 22.2500\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.5135 - val_loss: 22.2065\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 481us/step - loss: 0.5368 - val_loss: 22.1714\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 477us/step - loss: 0.4985 - val_loss: 22.1173\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 487us/step - loss: 0.4947 - val_loss: 22.0617\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.4960 - val_loss: 22.0284\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 461us/step - loss: 0.4758 - val_loss: 21.9879\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 460us/step - loss: 0.5221 - val_loss: 21.9438\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.5042 - val_loss: 21.9057\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.4928 - val_loss: 21.8675\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 456us/step - loss: 0.5184 - val_loss: 21.8375\n",
      "121/121 [==============================] - 0s 279us/step - loss: 1.0268\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 684us/step - loss: 7.5983 - val_loss: 5.1257\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 4.4539 - val_loss: 3.4757\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 460us/step - loss: 2.7857 - val_loss: 2.6119\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 1.9864 - val_loss: 2.0466\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 486us/step - loss: 1.4547 - val_loss: 1.7673\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 1.2097 - val_loss: 1.6195\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 482us/step - loss: 0.9854 - val_loss: 1.4459\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 474us/step - loss: 0.8983 - val_loss: 1.3155\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.8053 - val_loss: 1.2219\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 477us/step - loss: 0.8153 - val_loss: 1.2287\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 476us/step - loss: 0.7648 - val_loss: 1.2045\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.7289 - val_loss: 1.1797\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 469us/step - loss: 0.7296 - val_loss: 1.1161\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.7153 - val_loss: 1.1011\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.6690 - val_loss: 1.0810\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.6774 - val_loss: 1.0400\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.6946 - val_loss: 1.0145\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.6815 - val_loss: 1.0170\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.6614 - val_loss: 0.9758\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 467us/step - loss: 0.6475 - val_loss: 0.9805\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 544us/step - loss: 0.6361 - val_loss: 0.9829\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 0.6821 - val_loss: 1.0022\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.6371 - val_loss: 0.9774\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.6567 - val_loss: 0.9428\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.6541 - val_loss: 0.9642\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 461us/step - loss: 0.6350 - val_loss: 0.9354\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.6293 - val_loss: 0.9677\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.6107 - val_loss: 0.9845\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.6084 - val_loss: 0.9193\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 472us/step - loss: 0.6413 - val_loss: 0.9467\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 0.5970 - val_loss: 0.9005\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 532us/step - loss: 0.6293 - val_loss: 0.9272\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.5833 - val_loss: 0.8990\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 481us/step - loss: 0.6214 - val_loss: 0.8699\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.6472 - val_loss: 0.9055\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.5835 - val_loss: 0.8655\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 466us/step - loss: 0.6057 - val_loss: 0.8628\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 466us/step - loss: 0.5984 - val_loss: 0.8449\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.5801 - val_loss: 0.8198\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.6065 - val_loss: 0.8284\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 481us/step - loss: 0.5698 - val_loss: 0.8013\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 462us/step - loss: 0.5952 - val_loss: 0.8374\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.5780 - val_loss: 0.8480\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.5374 - val_loss: 0.8447\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.5648 - val_loss: 0.8192\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 465us/step - loss: 0.5914 - val_loss: 0.8292\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 0.5951 - val_loss: 0.8105\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 486us/step - loss: 0.5541 - val_loss: 0.8385\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.5743 - val_loss: 0.8085\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.5879 - val_loss: 0.8427\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 455us/step - loss: 0.5449 - val_loss: 0.8181\n",
      "121/121 [==============================] - 0s 263us/step - loss: 0.5706\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 748us/step - loss: 3.9933 - val_loss: 1.8870\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.8585 - val_loss: 0.8692\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 595us/step - loss: 0.6770 - val_loss: 0.6615\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.6407 - val_loss: 0.5799\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.5849 - val_loss: 0.5444\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 537us/step - loss: 0.5561 - val_loss: 0.5117\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.5373 - val_loss: 0.4858\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.5187 - val_loss: 0.4682\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.4905 - val_loss: 0.4462\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 530us/step - loss: 0.4545 - val_loss: 0.4389\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4197 - val_loss: 0.4231\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.4436 - val_loss: 0.4147\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4280 - val_loss: 0.4088\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 0.4067 - val_loss: 0.4079\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3996 - val_loss: 0.3958\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.3978 - val_loss: 0.3936\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3979 - val_loss: 0.3927\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.4001 - val_loss: 0.4017\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3698 - val_loss: 0.4109\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3917 - val_loss: 0.3940\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 526us/step - loss: 0.3947 - val_loss: 0.3873\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3725 - val_loss: 0.3866\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3831 - val_loss: 0.3872\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 526us/step - loss: 0.3783 - val_loss: 0.3899\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 571us/step - loss: 0.3610 - val_loss: 0.3976\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.3681 - val_loss: 0.3959\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3664 - val_loss: 0.3887\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3659 - val_loss: 0.3977\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3650 - val_loss: 0.3943\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3711 - val_loss: 0.4008\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.3728 - val_loss: 0.3814\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.3636 - val_loss: 0.3773\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3529 - val_loss: 0.3892\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3611 - val_loss: 0.3877\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3661 - val_loss: 0.3743\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 567us/step - loss: 0.3589 - val_loss: 0.3754\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3566 - val_loss: 0.3697\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3463 - val_loss: 0.3772\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.3621 - val_loss: 0.3699\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3598 - val_loss: 0.3753\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.3527 - val_loss: 0.3751\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3600 - val_loss: 0.3681\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.3339 - val_loss: 0.3672\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.3451 - val_loss: 0.3684\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3494 - val_loss: 0.3664\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3602 - val_loss: 0.3748\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 544us/step - loss: 0.3429 - val_loss: 0.3712\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3279 - val_loss: 0.3835\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3393 - val_loss: 0.3666\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3498 - val_loss: 0.3704\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3719 - val_loss: 0.3626\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3749 - val_loss: 0.3636\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3619 - val_loss: 0.3733\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.3545 - val_loss: 0.3738\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.3404 - val_loss: 0.3731\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.3548 - val_loss: 0.3807\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3541 - val_loss: 0.3806\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3451 - val_loss: 0.3625\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 556us/step - loss: 0.3403 - val_loss: 0.3569\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.3580 - val_loss: 0.3641\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3419 - val_loss: 0.3665\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.3307 - val_loss: 0.3684\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3429 - val_loss: 0.3555\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.3339 - val_loss: 0.3580\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.3501 - val_loss: 0.3617\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3368 - val_loss: 0.3627\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.3353 - val_loss: 0.3687\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.3436 - val_loss: 0.3539\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3375 - val_loss: 0.3520\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3326 - val_loss: 0.3518\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 536us/step - loss: 0.3440 - val_loss: 0.3574\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3445 - val_loss: 0.3527\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3413 - val_loss: 0.3482\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3442 - val_loss: 0.3506\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.3318 - val_loss: 0.3659\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3417 - val_loss: 0.3492\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3504 - val_loss: 0.3519\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.3273 - val_loss: 0.3617\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3360 - val_loss: 0.3579\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3414 - val_loss: 0.3502\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3445 - val_loss: 0.3436\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 550us/step - loss: 0.3323 - val_loss: 0.3567\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.3461 - val_loss: 0.3494\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.3502 - val_loss: 0.3624\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.3501 - val_loss: 0.3641\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3342 - val_loss: 0.3463\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.3559 - val_loss: 0.3377\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.3259 - val_loss: 0.3479\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3471 - val_loss: 0.3546\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3255 - val_loss: 0.3607\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3291 - val_loss: 0.3423\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 556us/step - loss: 0.3387 - val_loss: 0.3422\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.3371 - val_loss: 0.3575\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3206 - val_loss: 0.3639\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3368 - val_loss: 0.3547\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3415 - val_loss: 0.3541\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3318 - val_loss: 0.3613\n",
      "121/121 [==============================] - 0s 266us/step - loss: 0.3616\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 745us/step - loss: 3.1646 - val_loss: 16.8742\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.9296 - val_loss: 11.5526\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.7693 - val_loss: 6.7886\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 560us/step - loss: 0.7336 - val_loss: 3.8860\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.6783 - val_loss: 2.1255\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.6425 - val_loss: 1.1872\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.5904 - val_loss: 0.7286\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.5567 - val_loss: 0.5436\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.5621 - val_loss: 0.5179\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.5320 - val_loss: 0.5655\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.5210 - val_loss: 0.6710\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.4906 - val_loss: 0.7008\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.4964 - val_loss: 0.7454\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 534us/step - loss: 0.4789 - val_loss: 0.8059\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.4797 - val_loss: 0.8014\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4610 - val_loss: 0.7777\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.4496 - val_loss: 0.7492\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.4353 - val_loss: 0.7110\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4494 - val_loss: 0.6712\n",
      "121/121 [==============================] - 0s 268us/step - loss: 0.4569\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 755us/step - loss: 2.8152 - val_loss: 1.1574\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.8653 - val_loss: 0.7591\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 558us/step - loss: 0.7185 - val_loss: 0.7024\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.6945 - val_loss: 0.7191\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 593us/step - loss: 0.6419 - val_loss: 0.6038\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 521us/step - loss: 0.5981 - val_loss: 0.5961\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.5869 - val_loss: 0.5503\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.5525 - val_loss: 0.5199\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 538us/step - loss: 0.5525 - val_loss: 0.5170\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.5055 - val_loss: 0.4786\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.4827 - val_loss: 0.4758\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4845 - val_loss: 0.4712\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.4698 - val_loss: 0.4684\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4655 - val_loss: 0.4279\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 530us/step - loss: 0.4476 - val_loss: 0.4326\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 568us/step - loss: 0.4398 - val_loss: 0.4365\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.4246 - val_loss: 0.4052\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 711us/step - loss: 0.4349 - val_loss: 0.4289\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.4080 - val_loss: 0.4211\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 625us/step - loss: 0.4245 - val_loss: 0.3946\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 580us/step - loss: 0.4281 - val_loss: 0.4199\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 560us/step - loss: 0.4296 - val_loss: 0.3841\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 549us/step - loss: 0.4243 - val_loss: 0.4328\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 545us/step - loss: 0.4069 - val_loss: 0.3893\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 0.4297 - val_loss: 0.3790\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3901 - val_loss: 0.4342\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.4061 - val_loss: 0.3752\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 562us/step - loss: 0.4043 - val_loss: 0.3820\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 559us/step - loss: 0.3911 - val_loss: 0.3824\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 538us/step - loss: 0.4035 - val_loss: 0.3701\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 545us/step - loss: 0.4033 - val_loss: 0.3797\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 550us/step - loss: 0.3832 - val_loss: 0.4542\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 552us/step - loss: 0.3939 - val_loss: 0.3656\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 547us/step - loss: 0.3689 - val_loss: 0.4207\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 641us/step - loss: 0.3852 - val_loss: 0.3643\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 545us/step - loss: 0.3694 - val_loss: 0.3953\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3847 - val_loss: 0.3608\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 626us/step - loss: 0.3710 - val_loss: 0.3740\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.3734 - val_loss: 0.3682\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 615us/step - loss: 0.3786 - val_loss: 0.4032\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 577us/step - loss: 0.3692 - val_loss: 0.3771\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 596us/step - loss: 0.3650 - val_loss: 0.3947\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 650us/step - loss: 0.3716 - val_loss: 0.3644\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 737us/step - loss: 0.3782 - val_loss: 0.3562\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 651us/step - loss: 0.3722 - val_loss: 0.3651\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 630us/step - loss: 0.3848 - val_loss: 0.4238\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 726us/step - loss: 0.3882 - val_loss: 0.3529\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 555us/step - loss: 0.3640 - val_loss: 0.3973\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 584us/step - loss: 0.3499 - val_loss: 0.3610\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.3767 - val_loss: 0.3603\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 587us/step - loss: 0.3456 - val_loss: 0.3957\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.3717 - val_loss: 0.3693\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 604us/step - loss: 0.3556 - val_loss: 0.4445\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 583us/step - loss: 0.3643 - val_loss: 0.3812\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 590us/step - loss: 0.3605 - val_loss: 0.3828\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 606us/step - loss: 0.3745 - val_loss: 0.4378\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 0.3549 - val_loss: 0.3601\n",
      "121/121 [==============================] - 0s 301us/step - loss: 0.3680\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 806us/step - loss: 3.3243 - val_loss: 1.1832\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 597us/step - loss: 0.7785 - val_loss: 0.5895\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 564us/step - loss: 0.6153 - val_loss: 0.5415\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.5577 - val_loss: 0.5155\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.5343 - val_loss: 0.4969\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 480us/step - loss: 0.5307 - val_loss: 0.4815\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.5178 - val_loss: 0.4682\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4940 - val_loss: 0.4566\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 534us/step - loss: 0.4944 - val_loss: 0.4497\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.4910 - val_loss: 0.4395\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 617us/step - loss: 0.4580 - val_loss: 0.4340\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.4671 - val_loss: 0.4329\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.4650 - val_loss: 0.4284\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4423 - val_loss: 0.4258\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.4480 - val_loss: 0.4287\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4291 - val_loss: 0.4288\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.4378 - val_loss: 0.4360\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4253 - val_loss: 0.4250\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.4157 - val_loss: 0.4200\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.4322 - val_loss: 0.4107\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.4455 - val_loss: 0.4196\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.4258 - val_loss: 0.4153\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4352 - val_loss: 0.4296\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.4368 - val_loss: 0.4217\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.4206 - val_loss: 0.4332\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.4165 - val_loss: 0.4202\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.4065 - val_loss: 0.4354\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 611us/step - loss: 0.4207 - val_loss: 0.4250\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 589us/step - loss: 0.3988 - val_loss: 0.4400\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 560us/step - loss: 0.4041 - val_loss: 0.4238\n",
      "121/121 [==============================] - 0s 304us/step - loss: 0.4147\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 722us/step - loss: 3.1086 - val_loss: 2.9427\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.9260 - val_loss: 1.1169\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.7525 - val_loss: 0.6479\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.6543 - val_loss: 0.8268\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.5998 - val_loss: 1.1924\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 526us/step - loss: 0.5818 - val_loss: 1.5059\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.5219 - val_loss: 1.7998\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.5037 - val_loss: 1.9387\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 478us/step - loss: 0.4901 - val_loss: 2.0605\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 487us/step - loss: 0.4848 - val_loss: 2.0164\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4659 - val_loss: 1.9260\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.4466 - val_loss: 1.8465\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4553 - val_loss: 1.7150\n",
      "121/121 [==============================] - 0s 275us/step - loss: 0.5012\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 720us/step - loss: 3.7287 - val_loss: 4.2469\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 534us/step - loss: 0.8755 - val_loss: 1.8548\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.6973 - val_loss: 1.3970\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 480us/step - loss: 0.6508 - val_loss: 1.0291\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.5915 - val_loss: 0.7802\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.5776 - val_loss: 0.7358\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.5425 - val_loss: 0.5458\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.5013 - val_loss: 0.5237\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.5064 - val_loss: 0.4629\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.4864 - val_loss: 0.4512\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 532us/step - loss: 0.4787 - val_loss: 0.4544\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.4674 - val_loss: 0.4737\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.4612 - val_loss: 0.4284\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4628 - val_loss: 0.4318\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4729 - val_loss: 0.4311\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4492 - val_loss: 0.4117\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.4459 - val_loss: 0.4083\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 480us/step - loss: 0.4249 - val_loss: 0.4247\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.4338 - val_loss: 0.4002\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.4426 - val_loss: 0.3991\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 481us/step - loss: 0.4428 - val_loss: 0.3972\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.4287 - val_loss: 0.4813\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.4156 - val_loss: 0.3937\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.4242 - val_loss: 0.4039\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 486us/step - loss: 0.4234 - val_loss: 0.4281\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.4132 - val_loss: 0.4477\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 478us/step - loss: 0.4225 - val_loss: 0.3883\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4138 - val_loss: 0.4006\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.3944 - val_loss: 0.4015\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.4168 - val_loss: 0.3992\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4278 - val_loss: 0.3804\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 544us/step - loss: 0.4194 - val_loss: 0.4386\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 468us/step - loss: 0.4394 - val_loss: 0.3819\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.4297 - val_loss: 0.3835\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4177 - val_loss: 0.4370\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.3809 - val_loss: 0.3931\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.4260 - val_loss: 0.3941\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.4000 - val_loss: 0.3737\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4072 - val_loss: 0.4409\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.3962 - val_loss: 0.5040\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 483us/step - loss: 0.3838 - val_loss: 0.4983\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 538us/step - loss: 0.3962 - val_loss: 0.5198\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.4086 - val_loss: 0.7101\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.3931 - val_loss: 0.4774\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 478us/step - loss: 0.4104 - val_loss: 0.5284\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3806 - val_loss: 0.4254\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.4147 - val_loss: 0.5645\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 487us/step - loss: 0.3886 - val_loss: 0.3669\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.3798 - val_loss: 0.4236\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3990 - val_loss: 0.3694\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.3877 - val_loss: 0.3733\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.3879 - val_loss: 0.4433\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.3865 - val_loss: 0.3874\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 479us/step - loss: 0.3755 - val_loss: 0.4349\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 482us/step - loss: 0.3845 - val_loss: 0.3725\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3745 - val_loss: 0.4550\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.3922 - val_loss: 0.3606\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4026 - val_loss: 0.3960\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.3893 - val_loss: 0.3919\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 480us/step - loss: 0.3814 - val_loss: 0.4335\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 473us/step - loss: 0.3861 - val_loss: 0.3767\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3798 - val_loss: 0.4249\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.3788 - val_loss: 0.3755\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.3829 - val_loss: 0.3618\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.3887 - val_loss: 0.4541\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3747 - val_loss: 0.4521\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3811 - val_loss: 0.7427\n",
      "121/121 [==============================] - 0s 307us/step - loss: 0.3755\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 723us/step - loss: 1.7906 - val_loss: 1.5526\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.6998 - val_loss: 1.0449\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.6040 - val_loss: 2.6239\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.5165 - val_loss: 2.6681\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.4853 - val_loss: 2.1688\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 487us/step - loss: 0.4490 - val_loss: 0.6800\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.4470 - val_loss: 0.6148\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4370 - val_loss: 0.4050\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.4488 - val_loss: 0.4959\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 535us/step - loss: 0.4230 - val_loss: 0.4389\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.3990 - val_loss: 0.4080\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4094 - val_loss: 0.3992\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.4014 - val_loss: 0.4160\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.4146 - val_loss: 0.3775\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.3911 - val_loss: 0.4280\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.4106 - val_loss: 0.3924\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3709 - val_loss: 0.3695\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.3763 - val_loss: 0.4473\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3869 - val_loss: 0.3766\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 543us/step - loss: 0.3829 - val_loss: 0.3625\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 481us/step - loss: 0.3855 - val_loss: 0.4599\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.3661 - val_loss: 0.3651\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3800 - val_loss: 0.4234\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.3774 - val_loss: 0.3713\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.3803 - val_loss: 0.3542\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.3827 - val_loss: 0.3649\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.3659 - val_loss: 0.3495\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3788 - val_loss: 0.3565\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.3829 - val_loss: 0.4101\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 549us/step - loss: 0.3429 - val_loss: 0.4077\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.3641 - val_loss: 0.6197\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 483us/step - loss: 0.3486 - val_loss: 0.3600\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.3344 - val_loss: 0.5585\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3608 - val_loss: 0.3994\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3485 - val_loss: 0.4003\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 482us/step - loss: 0.3454 - val_loss: 0.3533\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3678 - val_loss: 0.3661\n",
      "121/121 [==============================] - 0s 279us/step - loss: 0.3760\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 1.7417 - val_loss: 2.3600\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 535us/step - loss: 0.6505 - val_loss: 0.5989\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.5731 - val_loss: 0.9964\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 527us/step - loss: 0.5410 - val_loss: 1.1041\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.5097 - val_loss: 0.9203\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.4840 - val_loss: 0.6959\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.4607 - val_loss: 0.4660\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4469 - val_loss: 0.4148\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4246 - val_loss: 0.4540\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4152 - val_loss: 0.5265\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 544us/step - loss: 0.4231 - val_loss: 0.6884\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.4208 - val_loss: 0.8312\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 482us/step - loss: 0.3910 - val_loss: 0.8435\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4086 - val_loss: 1.0575\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4070 - val_loss: 1.1171\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.3899 - val_loss: 1.3054\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3934 - val_loss: 1.2949\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.3973 - val_loss: 1.3799\n",
      "121/121 [==============================] - 0s 279us/step - loss: 0.4164\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 752us/step - loss: 2.2353 - val_loss: 49.3725\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 1.0652 - val_loss: 2.1454\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.5520 - val_loss: 0.4863\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4847 - val_loss: 0.4407\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.4655 - val_loss: 0.4122\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.4180 - val_loss: 0.4179\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 543us/step - loss: 0.4177 - val_loss: 0.4265\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4205 - val_loss: 0.4534\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.4010 - val_loss: 0.4151\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3905 - val_loss: 0.4267\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.4021 - val_loss: 0.3957\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.4040 - val_loss: 0.3824\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.3892 - val_loss: 0.3932\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 537us/step - loss: 0.3897 - val_loss: 0.4134\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.3969 - val_loss: 0.4144\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.4026 - val_loss: 0.4272\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.3790 - val_loss: 0.4337\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.3798 - val_loss: 0.3980\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.3747 - val_loss: 0.4100\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3719 - val_loss: 0.4284\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 540us/step - loss: 0.4122 - val_loss: 0.3630\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3887 - val_loss: 0.4347\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.3601 - val_loss: 0.3549\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3742 - val_loss: 0.4041\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 486us/step - loss: 0.3804 - val_loss: 0.3544\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.3707 - val_loss: 0.3631\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3739 - val_loss: 0.4474\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.3530 - val_loss: 0.3742\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.3715 - val_loss: 0.3514\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 0.3790 - val_loss: 0.3946\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.3680 - val_loss: 0.3469\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 512us/step - loss: 0.3552 - val_loss: 0.3590\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 488us/step - loss: 0.3746 - val_loss: 0.3773\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3654 - val_loss: 0.3703\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3738 - val_loss: 0.3617\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.3787 - val_loss: 0.3649\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 554us/step - loss: 0.3777 - val_loss: 0.3439\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3578 - val_loss: 0.4293\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3690 - val_loss: 0.3622\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.3478 - val_loss: 0.3810\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.3632 - val_loss: 0.3477\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.3564 - val_loss: 0.3549\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3639 - val_loss: 0.3450\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.3578 - val_loss: 0.4176\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3586 - val_loss: 0.3665\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.3607 - val_loss: 0.3494\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3676 - val_loss: 0.3478\n",
      "121/121 [==============================] - 0s 281us/step - loss: 0.3553\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 733us/step - loss: 2.6282 - val_loss: 17.4045\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 523us/step - loss: 0.9129 - val_loss: 12.9390\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.6988 - val_loss: 3.6982\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 579us/step - loss: 0.6024 - val_loss: 1.7920\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 557us/step - loss: 0.5623 - val_loss: 0.7675\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.5281 - val_loss: 0.5118\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 508us/step - loss: 0.5076 - val_loss: 0.4646\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 559us/step - loss: 0.5020 - val_loss: 0.5070\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4495 - val_loss: 0.4374\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.4508 - val_loss: 0.4332\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4510 - val_loss: 0.4214\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.4521 - val_loss: 0.4095\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4208 - val_loss: 0.4017\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 587us/step - loss: 0.4253 - val_loss: 0.4124\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 590us/step - loss: 0.4251 - val_loss: 0.3941\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 529us/step - loss: 0.4058 - val_loss: 0.3902\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4269 - val_loss: 0.3860\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.4007 - val_loss: 0.3866\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.4021 - val_loss: 0.4065\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.4340 - val_loss: 0.3773\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 509us/step - loss: 0.3827 - val_loss: 0.3767\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4223 - val_loss: 0.3813\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3888 - val_loss: 0.3921\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.3948 - val_loss: 0.3776\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3974 - val_loss: 0.3763\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 552us/step - loss: 0.3787 - val_loss: 0.3873\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.4196 - val_loss: 0.3689\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3998 - val_loss: 0.4139\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 0.3919 - val_loss: 0.3962\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 594us/step - loss: 0.3843 - val_loss: 0.4812\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4011 - val_loss: 0.3818\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 575us/step - loss: 0.4012 - val_loss: 0.3896\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 565us/step - loss: 0.3736 - val_loss: 0.3735\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 0.3990 - val_loss: 0.4138\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.3879 - val_loss: 0.4067\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3739 - val_loss: 0.6215\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3778 - val_loss: 0.5070\n",
      "121/121 [==============================] - 0s 290us/step - loss: 0.3939\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 772us/step - loss: 3.9892 - val_loss: 1.4053\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 569us/step - loss: 0.7218 - val_loss: 0.6426\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.6441 - val_loss: 0.6000\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.5630 - val_loss: 0.7284\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.5479 - val_loss: 0.9870\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.5304 - val_loss: 1.2004\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.5088 - val_loss: 1.4536\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.4674 - val_loss: 1.5395\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 550us/step - loss: 0.4641 - val_loss: 1.5590\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.4590 - val_loss: 1.5572\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.4532 - val_loss: 1.4521\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.4483 - val_loss: 1.3857\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.4256 - val_loss: 1.2206\n",
      "121/121 [==============================] - 0s 277us/step - loss: 0.4676\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 762us/step - loss: 2.7008 - val_loss: 1.3946\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.6895 - val_loss: 0.6255\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.6070 - val_loss: 0.5630\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.5679 - val_loss: 0.5308\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.5515 - val_loss: 0.5047\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 522us/step - loss: 0.5158 - val_loss: 0.4804\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.5024 - val_loss: 0.4685\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 524us/step - loss: 0.4775 - val_loss: 0.4533\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4774 - val_loss: 0.4404\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.4594 - val_loss: 0.4439\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 540us/step - loss: 0.4565 - val_loss: 0.4279\n",
      "Epoch 12/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.4601 - val_loss: 0.4467\n",
      "Epoch 13/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.4353 - val_loss: 0.4432\n",
      "Epoch 14/100\n",
      "242/242 [==============================] - 0s 539us/step - loss: 0.4361 - val_loss: 0.4349\n",
      "Epoch 15/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.4321 - val_loss: 0.4395\n",
      "Epoch 16/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.4047 - val_loss: 0.4070\n",
      "Epoch 17/100\n",
      "242/242 [==============================] - 0s 491us/step - loss: 0.4109 - val_loss: 0.4430\n",
      "Epoch 18/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 0.4599 - val_loss: 0.4307\n",
      "Epoch 19/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4216 - val_loss: 0.4236\n",
      "Epoch 20/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4291 - val_loss: 0.4283\n",
      "Epoch 21/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.4078 - val_loss: 0.3945\n",
      "Epoch 22/100\n",
      "242/242 [==============================] - 0s 506us/step - loss: 0.4079 - val_loss: 0.4412\n",
      "Epoch 23/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.4073 - val_loss: 0.4300\n",
      "Epoch 24/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.4130 - val_loss: 0.3938\n",
      "Epoch 25/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.4092 - val_loss: 0.4678\n",
      "Epoch 26/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.3890 - val_loss: 0.4253\n",
      "Epoch 27/100\n",
      "242/242 [==============================] - 0s 542us/step - loss: 0.4061 - val_loss: 0.3836\n",
      "Epoch 28/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.4102 - val_loss: 0.3890\n",
      "Epoch 29/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.4212 - val_loss: 0.3824\n",
      "Epoch 30/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.4142 - val_loss: 0.4547\n",
      "Epoch 31/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.4048 - val_loss: 0.4342\n",
      "Epoch 32/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.4068 - val_loss: 0.3812\n",
      "Epoch 33/100\n",
      "242/242 [==============================] - 0s 545us/step - loss: 0.4036 - val_loss: 0.4083\n",
      "Epoch 34/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.4146 - val_loss: 0.4222\n",
      "Epoch 35/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3927 - val_loss: 0.3806\n",
      "Epoch 36/100\n",
      "242/242 [==============================] - 0s 540us/step - loss: 0.3868 - val_loss: 0.3838\n",
      "Epoch 37/100\n",
      "242/242 [==============================] - 0s 528us/step - loss: 0.3981 - val_loss: 0.4440\n",
      "Epoch 38/100\n",
      "242/242 [==============================] - 0s 495us/step - loss: 0.3937 - val_loss: 0.3685\n",
      "Epoch 39/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3885 - val_loss: 0.3799\n",
      "Epoch 40/100\n",
      "242/242 [==============================] - 0s 519us/step - loss: 0.3960 - val_loss: 0.3695\n",
      "Epoch 41/100\n",
      "242/242 [==============================] - 0s 520us/step - loss: 0.3960 - val_loss: 0.4226\n",
      "Epoch 42/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 0.3818 - val_loss: 0.4173\n",
      "Epoch 43/100\n",
      "242/242 [==============================] - 0s 545us/step - loss: 0.3978 - val_loss: 0.4175\n",
      "Epoch 44/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.3933 - val_loss: 0.4247\n",
      "Epoch 45/100\n",
      "242/242 [==============================] - 0s 541us/step - loss: 0.3928 - val_loss: 0.3581\n",
      "Epoch 46/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.3727 - val_loss: 0.4443\n",
      "Epoch 47/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3861 - val_loss: 0.3623\n",
      "Epoch 48/100\n",
      "242/242 [==============================] - 0s 518us/step - loss: 0.3727 - val_loss: 0.4595\n",
      "Epoch 49/100\n",
      "242/242 [==============================] - 0s 532us/step - loss: 0.3966 - val_loss: 0.4047\n",
      "Epoch 50/100\n",
      "242/242 [==============================] - 0s 517us/step - loss: 0.3982 - val_loss: 0.4096\n",
      "Epoch 51/100\n",
      "242/242 [==============================] - 0s 568us/step - loss: 0.3739 - val_loss: 0.3572\n",
      "Epoch 52/100\n",
      "242/242 [==============================] - 0s 507us/step - loss: 0.3840 - val_loss: 0.3955\n",
      "Epoch 53/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.3997 - val_loss: 0.3885\n",
      "Epoch 54/100\n",
      "242/242 [==============================] - 0s 525us/step - loss: 0.3864 - val_loss: 0.3530\n",
      "Epoch 55/100\n",
      "242/242 [==============================] - 0s 669us/step - loss: 0.4007 - val_loss: 0.3738\n",
      "Epoch 56/100\n",
      "242/242 [==============================] - 0s 558us/step - loss: 0.3892 - val_loss: 0.3523\n",
      "Epoch 57/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3867 - val_loss: 0.4109\n",
      "Epoch 58/100\n",
      "242/242 [==============================] - 0s 555us/step - loss: 0.3906 - val_loss: 0.4088\n",
      "Epoch 59/100\n",
      "242/242 [==============================] - 0s 532us/step - loss: 0.3784 - val_loss: 0.3491\n",
      "Epoch 60/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3708 - val_loss: 0.4108\n",
      "Epoch 61/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.3726 - val_loss: 0.3565\n",
      "Epoch 62/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.3792 - val_loss: 0.3492\n",
      "Epoch 63/100\n",
      "242/242 [==============================] - 0s 496us/step - loss: 0.3860 - val_loss: 0.3496\n",
      "Epoch 64/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.3863 - val_loss: 0.4010\n",
      "Epoch 65/100\n",
      "242/242 [==============================] - 0s 535us/step - loss: 0.3739 - val_loss: 0.3473\n",
      "Epoch 66/100\n",
      "242/242 [==============================] - 0s 510us/step - loss: 0.3887 - val_loss: 0.3607\n",
      "Epoch 67/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3597 - val_loss: 0.4102\n",
      "Epoch 68/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3616 - val_loss: 0.3474\n",
      "Epoch 69/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.3742 - val_loss: 0.3598\n",
      "Epoch 70/100\n",
      "242/242 [==============================] - 0s 490us/step - loss: 0.3530 - val_loss: 0.4324\n",
      "Epoch 71/100\n",
      "242/242 [==============================] - 0s 478us/step - loss: 0.3736 - val_loss: 0.3608\n",
      "Epoch 72/100\n",
      "242/242 [==============================] - 0s 500us/step - loss: 0.3722 - val_loss: 0.3444\n",
      "Epoch 73/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.3784 - val_loss: 0.4111\n",
      "Epoch 74/100\n",
      "242/242 [==============================] - 0s 566us/step - loss: 0.3651 - val_loss: 0.3681\n",
      "Epoch 75/100\n",
      "242/242 [==============================] - 0s 492us/step - loss: 0.3802 - val_loss: 0.3464\n",
      "Epoch 76/100\n",
      "242/242 [==============================] - 0s 497us/step - loss: 0.3571 - val_loss: 0.3558\n",
      "Epoch 77/100\n",
      "242/242 [==============================] - 0s 511us/step - loss: 0.3809 - val_loss: 0.3436\n",
      "Epoch 78/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3974 - val_loss: 0.3887\n",
      "Epoch 79/100\n",
      "242/242 [==============================] - 0s 516us/step - loss: 0.3508 - val_loss: 0.3437\n",
      "Epoch 80/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3664 - val_loss: 0.3472\n",
      "Epoch 81/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3852 - val_loss: 0.4000\n",
      "Epoch 82/100\n",
      "242/242 [==============================] - 0s 530us/step - loss: 0.3641 - val_loss: 0.3627\n",
      "Epoch 83/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.3695 - val_loss: 0.3852\n",
      "Epoch 84/100\n",
      "242/242 [==============================] - 0s 493us/step - loss: 0.3663 - val_loss: 0.3781\n",
      "Epoch 85/100\n",
      "242/242 [==============================] - 0s 479us/step - loss: 0.3774 - val_loss: 0.3424\n",
      "Epoch 86/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3644 - val_loss: 0.3523\n",
      "Epoch 87/100\n",
      "242/242 [==============================] - 0s 502us/step - loss: 0.3735 - val_loss: 0.3905\n",
      "Epoch 88/100\n",
      "242/242 [==============================] - 0s 505us/step - loss: 0.3430 - val_loss: 0.3531\n",
      "Epoch 89/100\n",
      "242/242 [==============================] - 0s 514us/step - loss: 0.3811 - val_loss: 0.3557\n",
      "Epoch 90/100\n",
      "242/242 [==============================] - 0s 582us/step - loss: 0.3700 - val_loss: 0.3728\n",
      "Epoch 91/100\n",
      "242/242 [==============================] - 0s 485us/step - loss: 0.3797 - val_loss: 0.3412\n",
      "Epoch 92/100\n",
      "242/242 [==============================] - 0s 484us/step - loss: 0.3646 - val_loss: 0.4594\n",
      "Epoch 93/100\n",
      "242/242 [==============================] - 0s 489us/step - loss: 0.3768 - val_loss: 0.3473\n",
      "Epoch 94/100\n",
      "242/242 [==============================] - 0s 503us/step - loss: 0.3562 - val_loss: 0.3617\n",
      "Epoch 95/100\n",
      "242/242 [==============================] - 0s 504us/step - loss: 0.3644 - val_loss: 0.3488\n",
      "Epoch 96/100\n",
      "242/242 [==============================] - 0s 501us/step - loss: 0.3587 - val_loss: 0.3518\n",
      "Epoch 97/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.3564 - val_loss: 0.3920\n",
      "Epoch 98/100\n",
      "242/242 [==============================] - 0s 548us/step - loss: 0.3624 - val_loss: 0.3448\n",
      "Epoch 99/100\n",
      "242/242 [==============================] - 0s 498us/step - loss: 0.3624 - val_loss: 0.3559\n",
      "Epoch 100/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.3693 - val_loss: 0.3968\n",
      "121/121 [==============================] - 0s 266us/step - loss: 0.3634\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 704us/step - loss: 2.3712 - val_loss: 7.9824\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 513us/step - loss: 0.9828 - val_loss: 8.8234\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 466us/step - loss: 0.5867 - val_loss: 21.6511\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 472us/step - loss: 0.7326 - val_loss: 115.7749\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 469us/step - loss: 4.2354 - val_loss: 438.8336\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 537us/step - loss: 14.4452 - val_loss: 1320.3081\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 472us/step - loss: 34.5684 - val_loss: 4357.5161\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 477us/step - loss: 144.6546 - val_loss: 13701.7012\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 476us/step - loss: 29.6421 - val_loss: 42522.7070\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 829.4157 - val_loss: 136521.2188\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 3360.7260 - val_loss: 438671.4688\n",
      "121/121 [==============================] - 0s 313us/step - loss: 1157.5042\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 691us/step - loss: 2.1533 - val_loss: 12.8087\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 465us/step - loss: 0.5088 - val_loss: 15.6389\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.4852 - val_loss: 16.5718\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 494us/step - loss: 0.5107 - val_loss: 17.6842\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 477us/step - loss: 0.5006 - val_loss: 19.6601\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 521us/step - loss: 0.4897 - val_loss: 19.5835\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 472us/step - loss: 0.4933 - val_loss: 18.4091\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 475us/step - loss: 0.5155 - val_loss: 20.1952\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 478us/step - loss: 0.5100 - val_loss: 17.7542\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 479us/step - loss: 0.4945 - val_loss: 19.1471\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 0.4845 - val_loss: 20.6559\n",
      "121/121 [==============================] - 0s 328us/step - loss: 1.0067\n",
      "Epoch 1/100\n",
      "242/242 [==============================] - 0s 692us/step - loss: 3.0085 - val_loss: 41.0448\n",
      "Epoch 2/100\n",
      "242/242 [==============================] - 0s 483us/step - loss: 1.7420 - val_loss: 48.4178\n",
      "Epoch 3/100\n",
      "242/242 [==============================] - 0s 470us/step - loss: 1.3259 - val_loss: 108.4434\n",
      "Epoch 4/100\n",
      "242/242 [==============================] - 0s 464us/step - loss: 1.6052 - val_loss: 205.2330\n",
      "Epoch 5/100\n",
      "242/242 [==============================] - 0s 471us/step - loss: 1.0682 - val_loss: 209.3703\n",
      "Epoch 6/100\n",
      "242/242 [==============================] - 0s 544us/step - loss: 1.0497 - val_loss: 482.4492\n",
      "Epoch 7/100\n",
      "242/242 [==============================] - 0s 515us/step - loss: 4.7492 - val_loss: 871.0328\n",
      "Epoch 8/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 2.2423 - val_loss: 1687.2013\n",
      "Epoch 9/100\n",
      "242/242 [==============================] - 0s 474us/step - loss: 6.8311 - val_loss: 2731.1025\n",
      "Epoch 10/100\n",
      "242/242 [==============================] - 0s 463us/step - loss: 84.5404 - val_loss: 4721.7983\n",
      "Epoch 11/100\n",
      "242/242 [==============================] - 0s 499us/step - loss: 163.1462 - val_loss: 8615.1748\n",
      "121/121 [==============================] - 0s 293us/step - loss: 8.1020\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f557cff4dc0>, as the constructor either does not set or modifies parameter learning_rate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb Cell 61\u001b[0m in \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb#Y114sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m param_distribs \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb#Y114sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn_hidden\u001b[39m\u001b[39m\"\u001b[39m:[\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb#Y114sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mn_neurons\u001b[39m\u001b[39m\"\u001b[39m: np\u001b[39m.\u001b[39marange(\u001b[39m1\u001b[39m, \u001b[39m100\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb#Y114sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m\"\u001b[39m: reciprocal(\u001b[39m3e-4\u001b[39m,\u001b[39m3e-2\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb#Y114sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb#Y114sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m rnd_search_cv \u001b[39m=\u001b[39m RandomizedSearchCV(keras_reg, param_distribs, n_iter \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m, cv\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb#Y114sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m rnd_search_cv\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs \u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m, validation_data \u001b[39m=\u001b[39;49m (X_valid, y_valid), callbacks \u001b[39m=\u001b[39;49m [keras\u001b[39m.\u001b[39;49mcallbacks\u001b[39m.\u001b[39;49mEarlyStopping(patience\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)])\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/sklearn/model_selection/_search.py:921\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_params_ \u001b[39m=\u001b[39m results[\u001b[39m\"\u001b[39m\u001b[39mparams\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_index_]\n\u001b[1;32m    918\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrefit:\n\u001b[1;32m    919\u001b[0m     \u001b[39m# we clone again after setting params in case some\u001b[39;00m\n\u001b[1;32m    920\u001b[0m     \u001b[39m# of the params are estimators as well.\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_estimator_ \u001b[39m=\u001b[39m clone(\n\u001b[1;32m    922\u001b[0m         clone(base_estimator)\u001b[39m.\u001b[39;49mset_params(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbest_params_)\n\u001b[1;32m    923\u001b[0m     )\n\u001b[1;32m    924\u001b[0m     refit_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    925\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/sklearn/base.py:90\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     88\u001b[0m     param2 \u001b[39m=\u001b[39m params_set[name]\n\u001b[1;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m param1 \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m param2:\n\u001b[0;32m---> 90\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     91\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCannot clone object \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, as the constructor \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39meither does not set or modifies parameter \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (estimator, name)\n\u001b[1;32m     93\u001b[0m         )\n\u001b[1;32m     94\u001b[0m \u001b[39mreturn\u001b[39;00m new_object\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot clone object <tensorflow.python.keras.wrappers.scikit_learn.KerasRegressor object at 0x7f557cff4dc0>, as the constructor either does not set or modifies parameter learning_rate"
     ]
    }
   ],
   "source": [
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "param_distribs = {\n",
    "    \"n_hidden\":[0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100),\n",
    "    \"learning_rate\": reciprocal(3e-4,3e-2)\n",
    "}\n",
    "\n",
    "# rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter = 10, cv=3)\n",
    "# rnd_search_cv.fit(X_train, y_train, epochs = 100, validation_data = (X_valid, y_valid), callbacks = [keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questao 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test)  = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train_full[5000:] / 255, X_train_full[:5000] / 255\n",
    "y_train, y_valid = y_train_full[5000:], y_train_full[:5000]\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "input_layer = keras.layers.Input(shape = [28,28])\n",
    "hidden_1 = keras.layers.Dense(100,activation='relu')(input_layer)\n",
    "hidden2 = keras.layers.Dense(100, activation='relu')(hidden_1)\n",
    "concat = keras.layers.Concatenate()([input_layer, hidden2])\n",
    "output = keras.layers.Dense(10, activation='softmax')(concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-27 18:35:15.859777: I tensorflow/core/profiler/lib/profiler_session.cc:136] Profiler session initializing.\n",
      "2023-03-27 18:35:15.859798: I tensorflow/core/profiler/lib/profiler_session.cc:155] Profiler session started.\n",
      "2023-03-27 18:35:15.867353: I tensorflow/core/profiler/lib/profiler_session.cc:172] Profiler session tear down.\n",
      "2023-03-27 18:35:15.867463: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1487] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "run_log_dir = get_run_log_dir()\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(log_dir = run_log_dir)\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(patience=10)\n",
    "model.compile(optimizer='sgd',loss='sparse_categorical_crossentropy', metrics='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": " assertion failed: [Condition x == y did not hold element-wise:] [x (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [32 1] [y (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [32 28]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert (defined at tmp/ipykernel_7913/1873810409.py:1) ]] [Op:__inference_train_function_1007464]\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb Cell 69\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/pedro/Desktop/Estudos/DeepLearning/chapter10_keras.ipynb#Y125sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train,y_train, validation_data\u001b[39m=\u001b[39;49m(X_valid,y_valid),epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, callbacks\u001b[39m=\u001b[39;49m[tensorboard_cb, early_stopping_callback])\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[39m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1101\u001b[0m   \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[39mwith\u001b[39;00m trace\u001b[39m.\u001b[39mTrace(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name) \u001b[39mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    829\u001b[0m   compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py:888\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    884\u001b[0m     \u001b[39mpass\u001b[39;00m  \u001b[39m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[1;32m    885\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m     \u001b[39m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     \u001b[39m# stateless function.\u001b[39;00m\n\u001b[0;32m--> 888\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_stateless_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    889\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m   _, _, _, filtered_flat_args \u001b[39m=\u001b[39m \\\n\u001b[1;32m    891\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn\u001b[39m.\u001b[39m_function_spec\u001b[39m.\u001b[39mcanonicalize_function_inputs(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    892\u001b[0m           \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m   2943\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1919\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    556\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    557\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    558\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    559\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    560\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    561\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf2/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     60\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m:  assertion failed: [Condition x == y did not hold element-wise:] [x (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/Shape_1:0) = ] [32 1] [y (sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/strided_slice:0) = ] [32 28]\n\t [[node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/assert_equal_1/Assert/Assert (defined at tmp/ipykernel_7913/1873810409.py:1) ]] [Op:__inference_train_function_1007464]\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,y_train, validation_data=(X_valid,y_valid),epochs=100, callbacks=[tensorboard_cb, early_stopping_callback])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
